
======================================================================
  RUN: run_002
  Models  -> run_002/models/
  Visuals -> run_002/visualizations/
======================================================================


======================================================================
  RUNNING BASELINES
======================================================================

============================================================
üìä Baseline: HOLD (no LP)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Mean reward: 0.00 ¬± 0.00

============================================================
üìä Baseline: Fixed Width=1 (centered, always reallocate)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Action mapping: width=1 ‚Üí action_id=1
  Mean reward: 2446.67 ¬± 0.00

============================================================
üìä Baseline: Fixed Width=5 (centered, always reallocate)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Action mapping: width=5 ‚Üí action_id=3
  Mean reward: 1555.95 ¬± 0.00

============================================================
üìä Baseline: Fixed Width=10 (centered, always reallocate)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Action mapping: width=10 ‚Üí action_id=5
  Mean reward: 255.02 ¬± 0.00

======================================================================
  TRAINING ALL MODELS
======================================================================

============================================================
  Training PPO (Xu & Brini 2025 methodology)
============================================================
============================================================
üöÄ Paper-Based Uniswap v3 PPO Training
   Following Xu & Brini (2025) - arXiv:2501.07508
============================================================
  Data dir: training_data
  Parallel environments: 4
  Total timesteps: 500,000
  Action ticks: [0, 1, 3, 5, 10, 20, 40]

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
  Using extended features (36-dim observation, matching DQN)

üèãÔ∏è Creating training environments...
  ‚úÖ 4 training envs, 1 eval env created

üß† Creating PPO model...
  n_steps per env: 512
  batch_size: 256
Using cpu device

üèÉ Starting training...
============================================================
-----------------------------
| time/              |      |
|    fps             | 1770 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1870        |
|    iterations           | 2           |
|    time_elapsed         | 2           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010826081 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -0.327      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0786      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.572       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 2039        |
|    iterations           | 3           |
|    time_elapsed         | 3           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012767449 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -0.269      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0294     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.0877      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 2031        |
|    iterations           | 4           |
|    time_elapsed         | 4           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010339047 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -0.87       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0071     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.0561      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=831.04 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 831         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010355184 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -0.46       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.046      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.039       |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1302  |
|    iterations      | 5     |
|    time_elapsed    | 7     |
|    total_timesteps | 10240 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1361         |
|    iterations           | 6            |
|    time_elapsed         | 9            |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0094327275 |
|    clip_fraction        | 0.0746       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.9         |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0334      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0176      |
|    value_loss           | 0.0483       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1442        |
|    iterations           | 7           |
|    time_elapsed         | 9           |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009713888 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -0.0331     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.0689      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1467        |
|    iterations           | 8           |
|    time_elapsed         | 11          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.009672649 |
|    clip_fraction        | 0.0923      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0302     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.036       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1497        |
|    iterations           | 9           |
|    time_elapsed         | 12          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.009844435 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -0.842      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.036      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0565      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=1164.79 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.16e+03   |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01031841 |
|    clip_fraction        | 0.0934     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.85      |
|    explained_variance   | 0.466      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.021     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.0531     |
----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1286  |
|    iterations      | 10    |
|    time_elapsed    | 15    |
|    total_timesteps | 20480 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1328        |
|    iterations           | 11          |
|    time_elapsed         | 16          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.013140782 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0222     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.0725      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1360        |
|    iterations           | 12          |
|    time_elapsed         | 18          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010580151 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | -0.597      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0218     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.134       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1405        |
|    iterations           | 13          |
|    time_elapsed         | 18          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.010174535 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.817      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.0463      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1454        |
|    iterations           | 14          |
|    time_elapsed         | 19          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009563962 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.268      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.0555      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=1290.96 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.009640362 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0403     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0266      |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1326  |
|    iterations      | 15    |
|    time_elapsed    | 23    |
|    total_timesteps | 30720 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1343        |
|    iterations           | 16          |
|    time_elapsed         | 24          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011692838 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.162       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0498     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.0262      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1375        |
|    iterations           | 17          |
|    time_elapsed         | 25          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.012558792 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.0645      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0366     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.0406      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1405        |
|    iterations           | 18          |
|    time_elapsed         | 26          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.009921958 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | -0.539      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.0206      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1418        |
|    iterations           | 19          |
|    time_elapsed         | 27          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.012893932 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.21        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0405     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.0357      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=1347.68 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.35e+03    |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.008685782 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.0748      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0317     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.0556      |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1329  |
|    iterations      | 20    |
|    time_elapsed    | 30    |
|    total_timesteps | 40960 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1352        |
|    iterations           | 21          |
|    time_elapsed         | 31          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.009189356 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0684      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1370        |
|    iterations           | 22          |
|    time_elapsed         | 32          |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.009404624 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.172       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0226     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0525      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1395        |
|    iterations           | 23          |
|    time_elapsed         | 33          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.010423282 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | -0.201      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.103       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1421        |
|    iterations           | 24          |
|    time_elapsed         | 34          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.007688246 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | -0.103      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0118     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.0549      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=1381.45 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.38e+03    |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008880958 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | -0.0469     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.068       |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1359  |
|    iterations      | 25    |
|    time_elapsed    | 37    |
|    total_timesteps | 51200 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1376        |
|    iterations           | 26          |
|    time_elapsed         | 38          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.009477065 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0129     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.0279      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1387        |
|    iterations           | 27          |
|    time_elapsed         | 39          |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.015730996 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | -0.585      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0463     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.0236      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1409        |
|    iterations           | 28          |
|    time_elapsed         | 40          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.009469856 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | -0.0355     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0433     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.032       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1420        |
|    iterations           | 29          |
|    time_elapsed         | 41          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.008042494 |
|    clip_fraction        | 0.0817      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | -0.426      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.0217      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=1369.34 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.37e+03    |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.010270647 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.291       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0294     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.0579      |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 1366  |
|    iterations      | 30    |
|    time_elapsed    | 44    |
|    total_timesteps | 61440 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1378        |
|    iterations           | 31          |
|    time_elapsed         | 46          |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.008959798 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.397       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.0311      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1396        |
|    iterations           | 32          |
|    time_elapsed         | 46          |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.011742377 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.0454      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1404        |
|    iterations           | 33          |
|    time_elapsed         | 48          |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.006408998 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | -0.0336     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00307    |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.107       |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1422       |
|    iterations           | 34         |
|    time_elapsed         | 48         |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.01124617 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | -0.477     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.054     |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0264    |
|    value_loss           | 0.0325     |
----------------------------------------
Eval num_timesteps=70000, episode_reward=1355.84 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.36e+03    |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.008249536 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0352      |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 1380  |
|    iterations      | 35    |
|    time_elapsed    | 51    |
|    total_timesteps | 71680 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1393        |
|    iterations           | 36          |
|    time_elapsed         | 52          |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.010474444 |
|    clip_fraction        | 0.0967      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | -0.511      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0363     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0272      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1403        |
|    iterations           | 37          |
|    time_elapsed         | 54          |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.011807867 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.025       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0455     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.0157      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1415        |
|    iterations           | 38          |
|    time_elapsed         | 54          |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.010105674 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.04       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0465     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0265      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1430         |
|    iterations           | 39           |
|    time_elapsed         | 55           |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0069582867 |
|    clip_fraction        | 0.0762       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.0437       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0238      |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.0134      |
|    value_loss           | 0.0321       |
------------------------------------------
Eval num_timesteps=80000, episode_reward=1376.44 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.38e+03     |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0085583925 |
|    clip_fraction        | 0.0885       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.444        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.011       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0131      |
|    value_loss           | 0.0331       |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 1385  |
|    iterations      | 40    |
|    time_elapsed    | 59    |
|    total_timesteps | 81920 |
------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1396       |
|    iterations           | 41         |
|    time_elapsed         | 60         |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.00983605 |
|    clip_fraction        | 0.0772     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | -0.246     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0524    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.032      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1408        |
|    iterations           | 42          |
|    time_elapsed         | 61          |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.008534705 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.039       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1416        |
|    iterations           | 43          |
|    time_elapsed         | 62          |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.010289914 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.0379      |
-----------------------------------------
Eval num_timesteps=90000, episode_reward=1382.76 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.38e+03    |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.010224234 |
|    clip_fraction        | 0.0971      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.0677      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0348     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0887      |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1381  |
|    iterations      | 44    |
|    time_elapsed    | 65    |
|    total_timesteps | 90112 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1395        |
|    iterations           | 45          |
|    time_elapsed         | 66          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.007463373 |
|    clip_fraction        | 0.0692      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | -1.65       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0127     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.0462      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1409       |
|    iterations           | 46         |
|    time_elapsed         | 66         |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.01093644 |
|    clip_fraction        | 0.0933     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0149    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.0444     |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1418        |
|    iterations           | 47          |
|    time_elapsed         | 67          |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.008897491 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0325     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.0293      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1424        |
|    iterations           | 48          |
|    time_elapsed         | 69          |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.010184376 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.0234      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0184      |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=1360.23 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.36e+03    |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.010967759 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.0549      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0316     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.0282      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1394   |
|    iterations      | 49     |
|    time_elapsed    | 71     |
|    total_timesteps | 100352 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1405        |
|    iterations           | 50          |
|    time_elapsed         | 72          |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.007629188 |
|    clip_fraction        | 0.074       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | -0.649      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0183      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1413        |
|    iterations           | 51          |
|    time_elapsed         | 73          |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.007132408 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00481    |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0468      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1418        |
|    iterations           | 52          |
|    time_elapsed         | 75          |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.008169996 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.969      |
|    explained_variance   | -0.187      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.0299      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1430         |
|    iterations           | 53           |
|    time_elapsed         | 75           |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 0.0069036176 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.916       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0152      |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.0363       |
------------------------------------------
Eval num_timesteps=110000, episode_reward=1337.03 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.34e+03    |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.008373009 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.0502      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0202     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.0464      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1396   |
|    iterations      | 54     |
|    time_elapsed    | 79     |
|    total_timesteps | 110592 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1409        |
|    iterations           | 55          |
|    time_elapsed         | 79          |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.008854985 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.0902      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.096       |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1420         |
|    iterations           | 56           |
|    time_elapsed         | 80           |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0065754596 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.908       |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00774     |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.0121      |
|    value_loss           | 0.0369       |
------------------------------------------
---------------------------------------
| time/                   |           |
|    fps                  | 1428      |
|    iterations           | 57        |
|    time_elapsed         | 81        |
|    total_timesteps      | 116736    |
| train/                  |           |
|    approx_kl            | 0.0088838 |
|    clip_fraction        | 0.0872    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.962    |
|    explained_variance   | -0.0788   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0136   |
|    n_updates            | 560       |
|    policy_gradient_loss | -0.0149   |
|    value_loss           | 0.0519    |
---------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 58          |
|    time_elapsed         | 82          |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.009235817 |
|    clip_fraction        | 0.0982      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.0235      |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=1333.10 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.008102503 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0198     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0167      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1404   |
|    iterations      | 59     |
|    time_elapsed    | 86     |
|    total_timesteps | 120832 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1415         |
|    iterations           | 60           |
|    time_elapsed         | 86           |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0075564673 |
|    clip_fraction        | 0.0661       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.917       |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.024       |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.0257       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1420         |
|    iterations           | 61           |
|    time_elapsed         | 87           |
|    total_timesteps      | 124928       |
| train/                  |              |
|    approx_kl            | 0.0075760875 |
|    clip_fraction        | 0.0599       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.869       |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0216      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.0199       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 62          |
|    time_elapsed         | 88          |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.010181528 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | -0.0909     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.0257      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1434         |
|    iterations           | 63           |
|    time_elapsed         | 89           |
|    total_timesteps      | 129024       |
| train/                  |              |
|    approx_kl            | 0.0075193606 |
|    clip_fraction        | 0.0602       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.871       |
|    explained_variance   | 0.507        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0219      |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.0151      |
|    value_loss           | 0.023        |
------------------------------------------
Eval num_timesteps=130000, episode_reward=1338.25 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.34e+03   |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.00806156 |
|    clip_fraction        | 0.0825     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.971     |
|    explained_variance   | 0.539      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0267    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.0304     |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1409   |
|    iterations      | 64     |
|    time_elapsed    | 92     |
|    total_timesteps | 131072 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1415        |
|    iterations           | 65          |
|    time_elapsed         | 94          |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.010017076 |
|    clip_fraction        | 0.0864      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0225     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0767      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1425        |
|    iterations           | 66          |
|    time_elapsed         | 94          |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.011047904 |
|    clip_fraction        | 0.0974      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | -0.402      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.049      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.0254      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 67          |
|    time_elapsed         | 95          |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.006208311 |
|    clip_fraction        | 0.0635      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.967      |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0199     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.0321      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1441        |
|    iterations           | 68          |
|    time_elapsed         | 96          |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.009872779 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.177       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.0299      |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=1344.33 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.34e+03   |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.00858889 |
|    clip_fraction        | 0.0877     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.506      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0385    |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.0169     |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1416   |
|    iterations      | 69     |
|    time_elapsed    | 99     |
|    total_timesteps | 141312 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1423       |
|    iterations           | 70         |
|    time_elapsed         | 100        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.00917222 |
|    clip_fraction        | 0.0916     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0334    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.0244     |
----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1432         |
|    iterations           | 71           |
|    time_elapsed         | 101          |
|    total_timesteps      | 145408       |
| train/                  |              |
|    approx_kl            | 0.0071685817 |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.882       |
|    explained_variance   | 0.0662       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0263      |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.0266       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1436        |
|    iterations           | 72          |
|    time_elapsed         | 102         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.006644427 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0216     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.0303      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1440        |
|    iterations           | 73          |
|    time_elapsed         | 103         |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.009720646 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 0.431       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.0275      |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=1289.57 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.007426535 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0374      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1420   |
|    iterations      | 74     |
|    time_elapsed    | 106    |
|    total_timesteps | 151552 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1424        |
|    iterations           | 75          |
|    time_elapsed         | 107         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.006954194 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.944      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0305     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.0274      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1432         |
|    iterations           | 76           |
|    time_elapsed         | 108          |
|    total_timesteps      | 155648       |
| train/                  |              |
|    approx_kl            | 0.0095716035 |
|    clip_fraction        | 0.0919       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0145      |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.0186      |
|    value_loss           | 0.0736       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1440        |
|    iterations           | 77          |
|    time_elapsed         | 109         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.007037637 |
|    clip_fraction        | 0.0654      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.897      |
|    explained_variance   | -0.46       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.0438      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 78          |
|    time_elapsed         | 110         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.009104588 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0244     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0352      |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=1284.42 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.28e+03     |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0095544085 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.599        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0321      |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.0216      |
|    value_loss           | 0.0251       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 79     |
|    time_elapsed    | 113    |
|    total_timesteps | 161792 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 80          |
|    time_elapsed         | 114         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.007678294 |
|    clip_fraction        | 0.0963      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.938      |
|    explained_variance   | -0.661      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0349     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0191      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1437        |
|    iterations           | 81          |
|    time_elapsed         | 115         |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.008329912 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.971      |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.0227      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1443         |
|    iterations           | 82           |
|    time_elapsed         | 116          |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 0.0075645223 |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.787       |
|    explained_variance   | -0.532       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.022       |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.0189       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1447        |
|    iterations           | 83          |
|    time_elapsed         | 117         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.007251307 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.0441      |
-----------------------------------------
Eval num_timesteps=170000, episode_reward=1292.81 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.29e+03     |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0064909067 |
|    clip_fraction        | 0.0575       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.788       |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0347      |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.0184       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 84     |
|    time_elapsed    | 120    |
|    total_timesteps | 172032 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1433        |
|    iterations           | 85          |
|    time_elapsed         | 121         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.008817114 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0318     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.0302      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1436        |
|    iterations           | 86          |
|    time_elapsed         | 122         |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.008059647 |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00845    |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0415      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1443        |
|    iterations           | 87          |
|    time_elapsed         | 123         |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.008738533 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.969      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0271     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0663      |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=1307.11 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.31e+03    |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.007670088 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0216     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.0333      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1427   |
|    iterations      | 88     |
|    time_elapsed    | 126    |
|    total_timesteps | 180224 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1433         |
|    iterations           | 89           |
|    time_elapsed         | 127          |
|    total_timesteps      | 182272       |
| train/                  |              |
|    approx_kl            | 0.0066273455 |
|    clip_fraction        | 0.0766       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.834       |
|    explained_variance   | 0.0169       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.037       |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.0486       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1437        |
|    iterations           | 90          |
|    time_elapsed         | 128         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.010786366 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0275     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.0151      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1441        |
|    iterations           | 91          |
|    time_elapsed         | 129         |
|    total_timesteps      | 186368      |
| train/                  |             |
|    approx_kl            | 0.010070406 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 0.0705      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0195     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0145      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 92          |
|    time_elapsed         | 130         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.006663557 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.772      |
|    explained_variance   | -0.131      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0408     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.03        |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=1315.85 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.32e+03     |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0070119826 |
|    clip_fraction        | 0.0639       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.84        |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0296      |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 0.0225       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1428   |
|    iterations      | 93     |
|    time_elapsed    | 133    |
|    total_timesteps | 190464 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1433        |
|    iterations           | 94          |
|    time_elapsed         | 134         |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.007886387 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | -0.00543    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.021       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1438        |
|    iterations           | 95          |
|    time_elapsed         | 135         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.005806281 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1442        |
|    iterations           | 96          |
|    time_elapsed         | 136         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.009625734 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0177     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0303      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1447       |
|    iterations           | 97         |
|    time_elapsed         | 137        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.00815389 |
|    clip_fraction        | 0.0745     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.848     |
|    explained_variance   | 0.409      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0197    |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.0651     |
----------------------------------------
Eval num_timesteps=200000, episode_reward=1302.54 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.3e+03     |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.008828737 |
|    clip_fraction        | 0.0883      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | -1.26       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.024       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1430   |
|    iterations      | 98     |
|    time_elapsed    | 140    |
|    total_timesteps | 200704 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1438        |
|    iterations           | 99          |
|    time_elapsed         | 140         |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.008816412 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.0324      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1442        |
|    iterations           | 100         |
|    time_elapsed         | 141         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.008507074 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.904      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.0239      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1444        |
|    iterations           | 101         |
|    time_elapsed         | 143         |
|    total_timesteps      | 206848      |
| train/                  |             |
|    approx_kl            | 0.008762829 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | -0.0977     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0328     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0139      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1450        |
|    iterations           | 102         |
|    time_elapsed         | 144         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.009324726 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.195       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0309     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.023       |
-----------------------------------------
Eval num_timesteps=210000, episode_reward=1328.39 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.33e+03     |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0062900777 |
|    clip_fraction        | 0.0723       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.732       |
|    explained_variance   | -0.912       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0295      |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.0127       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1434   |
|    iterations      | 103    |
|    time_elapsed    | 147    |
|    total_timesteps | 210944 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1437         |
|    iterations           | 104          |
|    time_elapsed         | 148          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0052964883 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.783       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0228      |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.0215       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1440         |
|    iterations           | 105          |
|    time_elapsed         | 149          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.0072360756 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.693       |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0172      |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.0162      |
|    value_loss           | 0.0202       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1444         |
|    iterations           | 106          |
|    time_elapsed         | 150          |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 0.0053949165 |
|    clip_fraction        | 0.0555       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.65        |
|    explained_variance   | 0.478        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0125      |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.0348       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1448         |
|    iterations           | 107          |
|    time_elapsed         | 151          |
|    total_timesteps      | 219136       |
| train/                  |              |
|    approx_kl            | 0.0063655726 |
|    clip_fraction        | 0.066        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.736       |
|    explained_variance   | 0.284        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0186      |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.0264       |
------------------------------------------
Eval num_timesteps=220000, episode_reward=1325.51 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.008017439 |
|    clip_fraction        | 0.081       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0715      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1433   |
|    iterations      | 108    |
|    time_elapsed    | 154    |
|    total_timesteps | 221184 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1439        |
|    iterations           | 109         |
|    time_elapsed         | 155         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.008277016 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.742      |
|    explained_variance   | -0.0557     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0282     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.0332      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1444         |
|    iterations           | 110          |
|    time_elapsed         | 155          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.0067128455 |
|    clip_fraction        | 0.0732       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.758       |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0154      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.0153      |
|    value_loss           | 0.0409       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 111         |
|    time_elapsed         | 156         |
|    total_timesteps      | 227328      |
| train/                  |             |
|    approx_kl            | 0.008181345 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0305     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0206      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1450        |
|    iterations           | 112         |
|    time_elapsed         | 158         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.006741629 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | -0.229      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0155     |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0173      |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=1330.54 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.005550917 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0283     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.0233      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1437   |
|    iterations      | 113    |
|    time_elapsed    | 161    |
|    total_timesteps | 231424 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1440         |
|    iterations           | 114          |
|    time_elapsed         | 162          |
|    total_timesteps      | 233472       |
| train/                  |              |
|    approx_kl            | 0.0064406767 |
|    clip_fraction        | 0.075        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.711       |
|    explained_variance   | -0.298       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0189      |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.0156      |
|    value_loss           | 0.0166       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1444        |
|    iterations           | 115         |
|    time_elapsed         | 163         |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.007796899 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.719      |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.0328      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1447        |
|    iterations           | 116         |
|    time_elapsed         | 164         |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.007504627 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.636      |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0256     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0175      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1452         |
|    iterations           | 117          |
|    time_elapsed         | 164          |
|    total_timesteps      | 239616       |
| train/                  |              |
|    approx_kl            | 0.0057369703 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.635       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0127      |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.0317       |
------------------------------------------
Eval num_timesteps=240000, episode_reward=1338.62 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.34e+03    |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.007399477 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0066     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0757      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1437   |
|    iterations      | 118    |
|    time_elapsed    | 168    |
|    total_timesteps | 241664 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1442        |
|    iterations           | 119         |
|    time_elapsed         | 168         |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.006734298 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.722      |
|    explained_variance   | -0.312      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0351     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.0283      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1448         |
|    iterations           | 120          |
|    time_elapsed         | 169          |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0064413175 |
|    clip_fraction        | 0.058        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0.638        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.022       |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 0.0208       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1452        |
|    iterations           | 121         |
|    time_elapsed         | 170         |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.007425581 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.0419      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0281     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.0277      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1455        |
|    iterations           | 122         |
|    time_elapsed         | 171         |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.008013077 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.168       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0361     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.0127      |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=1336.31 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.34e+03     |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0059652524 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.683       |
|    explained_variance   | 0.0877       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.024       |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.0159       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1441   |
|    iterations      | 123    |
|    time_elapsed    | 174    |
|    total_timesteps | 251904 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1446         |
|    iterations           | 124          |
|    time_elapsed         | 175          |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.0098192245 |
|    clip_fraction        | 0.0709       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.66        |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0234      |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.0205       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 125         |
|    time_elapsed         | 176         |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.008054091 |
|    clip_fraction        | 0.0823      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0304     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.0238      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1451         |
|    iterations           | 126          |
|    time_elapsed         | 177          |
|    total_timesteps      | 258048       |
| train/                  |              |
|    approx_kl            | 0.0073715253 |
|    clip_fraction        | 0.0705       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.636       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0412      |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.0214       |
------------------------------------------
Eval num_timesteps=260000, episode_reward=1317.33 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.32e+03   |
| time/                   |            |
|    total_timesteps      | 260000     |
| train/                  |            |
|    approx_kl            | 0.00624477 |
|    clip_fraction        | 0.0556     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.605     |
|    explained_variance   | 0.52       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0168    |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 0.0256     |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1438   |
|    iterations      | 127    |
|    time_elapsed    | 180    |
|    total_timesteps | 260096 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1441        |
|    iterations           | 128         |
|    time_elapsed         | 181         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.008481856 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.025       |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1445         |
|    iterations           | 129          |
|    time_elapsed         | 182          |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 0.0071106143 |
|    clip_fraction        | 0.0698       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.751       |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0196      |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.0595       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1450        |
|    iterations           | 130         |
|    time_elapsed         | 183         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.009196412 |
|    clip_fraction        | 0.0861      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | -0.709      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0191     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.0285      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1455        |
|    iterations           | 131         |
|    time_elapsed         | 184         |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.007886017 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0373      |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=1282.84 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.007390742 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.024       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1442   |
|    iterations      | 132    |
|    time_elapsed    | 187    |
|    total_timesteps | 270336 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1443       |
|    iterations           | 133        |
|    time_elapsed         | 188        |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.00897623 |
|    clip_fraction        | 0.0793     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.741     |
|    explained_variance   | 0.067      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0277    |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.0163     |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 134         |
|    time_elapsed         | 189         |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.008272093 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0188     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0153      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1452       |
|    iterations           | 135        |
|    time_elapsed         | 190        |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.01025816 |
|    clip_fraction        | 0.0832     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.731     |
|    explained_variance   | -0.974     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0337    |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.016      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1454        |
|    iterations           | 136         |
|    time_elapsed         | 191         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.010795652 |
|    clip_fraction        | 0.0961      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0271     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0216      |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=1322.14 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.32e+03    |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.007856391 |
|    clip_fraction        | 0.0756      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.673      |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0222     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0143      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1440   |
|    iterations      | 137    |
|    time_elapsed    | 194    |
|    total_timesteps | 280576 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1445        |
|    iterations           | 138         |
|    time_elapsed         | 195         |
|    total_timesteps      | 282624      |
| train/                  |             |
|    approx_kl            | 0.004425236 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0129     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.0242      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1447        |
|    iterations           | 139         |
|    time_elapsed         | 196         |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.009465814 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.152       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.023      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0398      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1451         |
|    iterations           | 140          |
|    time_elapsed         | 197          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.0072783506 |
|    clip_fraction        | 0.0608       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.74        |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0174      |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.064        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1455        |
|    iterations           | 141         |
|    time_elapsed         | 198         |
|    total_timesteps      | 288768      |
| train/                  |             |
|    approx_kl            | 0.012218513 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.0288      |
-----------------------------------------
Eval num_timesteps=290000, episode_reward=1300.67 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.3e+03    |
| time/                   |            |
|    total_timesteps      | 290000     |
| train/                  |            |
|    approx_kl            | 0.00766542 |
|    clip_fraction        | 0.0673     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.735     |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0211    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.041      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1443   |
|    iterations      | 142    |
|    time_elapsed    | 201    |
|    total_timesteps | 290816 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1446         |
|    iterations           | 143          |
|    time_elapsed         | 202          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0111050885 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.848       |
|    explained_variance   | 0.462        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0358      |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.0214      |
|    value_loss           | 0.0183       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 144         |
|    time_elapsed         | 203         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.015094892 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.836      |
|    explained_variance   | -0.198      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0451     |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.0132      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1452         |
|    iterations           | 145          |
|    time_elapsed         | 204          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0063141044 |
|    clip_fraction        | 0.0702       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.663       |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0244      |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.023        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1454        |
|    iterations           | 146         |
|    time_elapsed         | 205         |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.009230234 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.87       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0287     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0197      |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=1255.44 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.008249373 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0241     |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0204      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1443   |
|    iterations      | 147    |
|    time_elapsed    | 208    |
|    total_timesteps | 301056 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1445        |
|    iterations           | 148         |
|    time_elapsed         | 209         |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.006252946 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0301     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.0173      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1449         |
|    iterations           | 149          |
|    time_elapsed         | 210          |
|    total_timesteps      | 305152       |
| train/                  |              |
|    approx_kl            | 0.0059953057 |
|    clip_fraction        | 0.0596       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.605       |
|    explained_variance   | 0.529        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0272      |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.0269       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1451        |
|    iterations           | 150         |
|    time_elapsed         | 211         |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.007935869 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0226     |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.0655      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1455        |
|    iterations           | 151         |
|    time_elapsed         | 212         |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.008817267 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.744      |
|    explained_variance   | -0.554      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0238     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.026       |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=1264.84 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.006123203 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0175     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.0271      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1444   |
|    iterations      | 152    |
|    time_elapsed    | 215    |
|    total_timesteps | 311296 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1447         |
|    iterations           | 153          |
|    time_elapsed         | 216          |
|    total_timesteps      | 313344       |
| train/                  |              |
|    approx_kl            | 0.0091231875 |
|    clip_fraction        | 0.0776       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.821       |
|    explained_variance   | 0.257        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0251      |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.0163      |
|    value_loss           | 0.0237       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1450        |
|    iterations           | 154         |
|    time_elapsed         | 217         |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.010520395 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.766      |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0378     |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.0116      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1452         |
|    iterations           | 155          |
|    time_elapsed         | 218          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0077103656 |
|    clip_fraction        | 0.0796       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.76        |
|    explained_variance   | 0.389        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0273      |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.0158      |
|    value_loss           | 0.0188       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1456        |
|    iterations           | 156         |
|    time_elapsed         | 219         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.008313201 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0273     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.0205      |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=1277.04 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.28e+03   |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.00887605 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.851     |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0234    |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.0272     |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1443   |
|    iterations      | 157    |
|    time_elapsed    | 222    |
|    total_timesteps | 321536 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1445         |
|    iterations           | 158          |
|    time_elapsed         | 223          |
|    total_timesteps      | 323584       |
| train/                  |              |
|    approx_kl            | 0.0063872235 |
|    clip_fraction        | 0.0557       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.643        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.027       |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.021        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 159         |
|    time_elapsed         | 224         |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.007576415 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0205     |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.029       |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1450         |
|    iterations           | 160          |
|    time_elapsed         | 225          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0076161255 |
|    clip_fraction        | 0.0734       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.671       |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.035       |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 0.0243       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1454         |
|    iterations           | 161          |
|    time_elapsed         | 226          |
|    total_timesteps      | 329728       |
| train/                  |              |
|    approx_kl            | 0.0063292487 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.756       |
|    explained_variance   | 0.0329       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.032       |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.0148      |
|    value_loss           | 0.0572       |
------------------------------------------
Eval num_timesteps=330000, episode_reward=1322.05 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.32e+03    |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.005963362 |
|    clip_fraction        | 0.0714      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.672      |
|    explained_variance   | -0.0588     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.0301      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1444   |
|    iterations      | 162    |
|    time_elapsed    | 229    |
|    total_timesteps | 331776 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1448         |
|    iterations           | 163          |
|    time_elapsed         | 230          |
|    total_timesteps      | 333824       |
| train/                  |              |
|    approx_kl            | 0.0062169805 |
|    clip_fraction        | 0.0639       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.467        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0204      |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.0262       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1450        |
|    iterations           | 164         |
|    time_elapsed         | 231         |
|    total_timesteps      | 335872      |
| train/                  |             |
|    approx_kl            | 0.009072019 |
|    clip_fraction        | 0.096       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.0196      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1451         |
|    iterations           | 165          |
|    time_elapsed         | 232          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0067678336 |
|    clip_fraction        | 0.0633       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.684       |
|    explained_variance   | -0.507       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0241      |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.0139       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1455         |
|    iterations           | 166          |
|    time_elapsed         | 233          |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.0076821344 |
|    clip_fraction        | 0.0692       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.61        |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0279      |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 0.0146       |
------------------------------------------
Eval num_timesteps=340000, episode_reward=1326.49 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.007853923 |
|    clip_fraction        | 0.0805      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.706      |
|    explained_variance   | -0.0818     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0294     |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0164      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1444   |
|    iterations      | 167    |
|    time_elapsed    | 236    |
|    total_timesteps | 342016 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1447         |
|    iterations           | 168          |
|    time_elapsed         | 237          |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0062071243 |
|    clip_fraction        | 0.0649       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.664       |
|    explained_variance   | 0.664        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0237      |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.0134      |
|    value_loss           | 0.0285       |
------------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1448       |
|    iterations           | 169        |
|    time_elapsed         | 238        |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.00682781 |
|    clip_fraction        | 0.0723     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.571     |
|    explained_variance   | 0.207      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0212    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.0141     |
----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1451         |
|    iterations           | 170          |
|    time_elapsed         | 239          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0067942915 |
|    clip_fraction        | 0.0611       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0.433        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0154      |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 0.0247       |
------------------------------------------
Eval num_timesteps=350000, episode_reward=1345.27 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.35e+03    |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.011092678 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.165       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0302      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1440   |
|    iterations      | 171    |
|    time_elapsed    | 243    |
|    total_timesteps | 350208 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1444        |
|    iterations           | 172         |
|    time_elapsed         | 243         |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.006861984 |
|    clip_fraction        | 0.0552      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.0437      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1448        |
|    iterations           | 173         |
|    time_elapsed         | 244         |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.007979649 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0205      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1451        |
|    iterations           | 174         |
|    time_elapsed         | 245         |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.009876762 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0222     |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.0327      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1453         |
|    iterations           | 175          |
|    time_elapsed         | 246          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.0075939666 |
|    clip_fraction        | 0.0745       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.723       |
|    explained_variance   | 0.542        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0299      |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.0128       |
------------------------------------------
Eval num_timesteps=360000, episode_reward=1361.18 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.36e+03    |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.006996152 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.698      |
|    explained_variance   | -0.229      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0124     |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.0114      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1442   |
|    iterations      | 176    |
|    time_elapsed    | 249    |
|    total_timesteps | 360448 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1446         |
|    iterations           | 177          |
|    time_elapsed         | 250          |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0072502065 |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0158      |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.019        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1447        |
|    iterations           | 178         |
|    time_elapsed         | 251         |
|    total_timesteps      | 364544      |
| train/                  |             |
|    approx_kl            | 0.009043559 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.0213      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1450       |
|    iterations           | 179        |
|    time_elapsed         | 252        |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.00998407 |
|    clip_fraction        | 0.0984     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0336    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.0146     |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1452        |
|    iterations           | 180         |
|    time_elapsed         | 253         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.008076088 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0125      |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=1353.21 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.35e+03    |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.005508206 |
|    clip_fraction        | 0.0602      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0348     |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0225      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1443   |
|    iterations      | 181    |
|    time_elapsed    | 256    |
|    total_timesteps | 370688 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1445        |
|    iterations           | 182         |
|    time_elapsed         | 257         |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.006670245 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0318     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0475      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1448         |
|    iterations           | 183          |
|    time_elapsed         | 258          |
|    total_timesteps      | 374784       |
| train/                  |              |
|    approx_kl            | 0.0067743203 |
|    clip_fraction        | 0.0737       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.661       |
|    explained_variance   | -1.17        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0287      |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.0191       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1452         |
|    iterations           | 184          |
|    time_elapsed         | 259          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0076306537 |
|    clip_fraction        | 0.0711       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.575       |
|    explained_variance   | 0.426        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0211      |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 0.024        |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1454         |
|    iterations           | 185          |
|    time_elapsed         | 260          |
|    total_timesteps      | 378880       |
| train/                  |              |
|    approx_kl            | 0.0063577755 |
|    clip_fraction        | 0.0544       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.683       |
|    explained_variance   | 0.668        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0146      |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 0.0174       |
------------------------------------------
Eval num_timesteps=380000, episode_reward=1367.34 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.37e+03    |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.005931507 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0164     |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.0105      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1443   |
|    iterations      | 186    |
|    time_elapsed    | 263    |
|    total_timesteps | 380928 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1446         |
|    iterations           | 187          |
|    time_elapsed         | 264          |
|    total_timesteps      | 382976       |
| train/                  |              |
|    approx_kl            | 0.0070808143 |
|    clip_fraction        | 0.0713       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.581       |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0246      |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.0135       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1449        |
|    iterations           | 188         |
|    time_elapsed         | 265         |
|    total_timesteps      | 385024      |
| train/                  |             |
|    approx_kl            | 0.008039216 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.64       |
|    explained_variance   | -0.469      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.036      |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1451        |
|    iterations           | 189         |
|    time_elapsed         | 266         |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.006962576 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.73       |
|    explained_variance   | 0.521       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0258     |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.0176      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1453        |
|    iterations           | 190         |
|    time_elapsed         | 267         |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.006736015 |
|    clip_fraction        | 0.0584      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.0168      |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=1298.94 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.3e+03     |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.008316351 |
|    clip_fraction        | 0.0705      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0263     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.0263      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1443   |
|    iterations      | 191    |
|    time_elapsed    | 271    |
|    total_timesteps | 391168 |
-------------------------------
---------------------------------------
| time/                   |           |
|    fps                  | 1442      |
|    iterations           | 192       |
|    time_elapsed         | 272       |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.0065301 |
|    clip_fraction        | 0.0705    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.592    |
|    explained_variance   | 0.392     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0215   |
|    n_updates            | 1910      |
|    policy_gradient_loss | -0.0136   |
|    value_loss           | 0.0248    |
---------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1443        |
|    iterations           | 193         |
|    time_elapsed         | 273         |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.008577869 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.0436      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.054       |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1443         |
|    iterations           | 194          |
|    time_elapsed         | 275          |
|    total_timesteps      | 397312       |
| train/                  |              |
|    approx_kl            | 0.0076888986 |
|    clip_fraction        | 0.085        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.225        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0277      |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.0287       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1445         |
|    iterations           | 195          |
|    time_elapsed         | 276          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.0052219154 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 0.47         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00982     |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.0106      |
|    value_loss           | 0.0283       |
------------------------------------------
Eval num_timesteps=400000, episode_reward=1283.99 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.28e+03   |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.00880138 |
|    clip_fraction        | 0.0957     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.772     |
|    explained_variance   | 0.467      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0245    |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.018      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1434   |
|    iterations      | 196    |
|    time_elapsed    | 279    |
|    total_timesteps | 401408 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1434         |
|    iterations           | 197          |
|    time_elapsed         | 281          |
|    total_timesteps      | 403456       |
| train/                  |              |
|    approx_kl            | 0.0068594944 |
|    clip_fraction        | 0.0682       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.652       |
|    explained_variance   | -0.413       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0189      |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.0127       |
------------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1437       |
|    iterations           | 198        |
|    time_elapsed         | 282        |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.00660384 |
|    clip_fraction        | 0.0673     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.56      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0268    |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.0161     |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1438        |
|    iterations           | 199         |
|    time_elapsed         | 283         |
|    total_timesteps      | 407552      |
| train/                  |             |
|    approx_kl            | 0.010402781 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.815      |
|    explained_variance   | -0.262      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0197     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.0191      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1440        |
|    iterations           | 200         |
|    time_elapsed         | 284         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.006881778 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0202     |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.0259      |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=1325.90 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.33e+03     |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0071253655 |
|    clip_fraction        | 0.0666       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.583       |
|    explained_variance   | 0.562        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0353      |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.0148      |
|    value_loss           | 0.0143       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1429   |
|    iterations      | 201    |
|    time_elapsed    | 287    |
|    total_timesteps | 411648 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 202         |
|    time_elapsed         | 288         |
|    total_timesteps      | 413696      |
| train/                  |             |
|    approx_kl            | 0.006042127 |
|    clip_fraction        | 0.0509      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.0263      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1433        |
|    iterations           | 203         |
|    time_elapsed         | 289         |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.007986996 |
|    clip_fraction        | 0.0777      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0569      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 204         |
|    time_elapsed         | 291         |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.008354446 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.684      |
|    explained_variance   | -0.686      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.042      |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0222      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1438        |
|    iterations           | 205         |
|    time_elapsed         | 291         |
|    total_timesteps      | 419840      |
| train/                  |             |
|    approx_kl            | 0.008590706 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0273     |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.0175      |
-----------------------------------------
Eval num_timesteps=420000, episode_reward=1312.62 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.31e+03     |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0078064874 |
|    clip_fraction        | 0.0818       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.734       |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0217      |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.021        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1429   |
|    iterations      | 206    |
|    time_elapsed    | 295    |
|    total_timesteps | 421888 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 207         |
|    time_elapsed         | 296         |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.008408897 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.63       |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0103     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.00964     |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1433         |
|    iterations           | 208          |
|    time_elapsed         | 297          |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0067027477 |
|    clip_fraction        | 0.0792       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.649       |
|    explained_variance   | 0.245        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0336      |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.0165      |
|    value_loss           | 0.0153       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 209         |
|    time_elapsed         | 298         |
|    total_timesteps      | 428032      |
| train/                  |             |
|    approx_kl            | 0.008048256 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0268     |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.0181      |
-----------------------------------------
Eval num_timesteps=430000, episode_reward=1281.20 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.28e+03     |
| time/                   |              |
|    total_timesteps      | 430000       |
| train/                  |              |
|    approx_kl            | 0.0076421183 |
|    clip_fraction        | 0.0827       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.825       |
|    explained_variance   | 0.482        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0339      |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.0188      |
|    value_loss           | 0.0252       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1425   |
|    iterations      | 210    |
|    time_elapsed    | 301    |
|    total_timesteps | 430080 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1427        |
|    iterations           | 211         |
|    time_elapsed         | 302         |
|    total_timesteps      | 432128      |
| train/                  |             |
|    approx_kl            | 0.007275499 |
|    clip_fraction        | 0.0752      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0194      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1428         |
|    iterations           | 212          |
|    time_elapsed         | 303          |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 0.0075983773 |
|    clip_fraction        | 0.0733       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.596        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0226      |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.0204       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 213         |
|    time_elapsed         | 305         |
|    total_timesteps      | 436224      |
| train/                  |             |
|    approx_kl            | 0.006404696 |
|    clip_fraction        | 0.0627      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0224      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 214         |
|    time_elapsed         | 306         |
|    total_timesteps      | 438272      |
| train/                  |             |
|    approx_kl            | 0.010854791 |
|    clip_fraction        | 0.0982      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0462      |
-----------------------------------------
Eval num_timesteps=440000, episode_reward=1311.29 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.31e+03    |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.009880472 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.733      |
|    explained_variance   | -0.363      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.033      |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.0217      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1423   |
|    iterations      | 215    |
|    time_elapsed    | 309    |
|    total_timesteps | 440320 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1426        |
|    iterations           | 216         |
|    time_elapsed         | 310         |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.007827021 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0194     |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.0246      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 217         |
|    time_elapsed         | 311         |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.008276977 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.868      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0372     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.017       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 218         |
|    time_elapsed         | 312         |
|    total_timesteps      | 446464      |
| train/                  |             |
|    approx_kl            | 0.008974864 |
|    clip_fraction        | 0.0909      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.0844      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0206     |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0115      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 219         |
|    time_elapsed         | 313         |
|    total_timesteps      | 448512      |
| train/                  |             |
|    approx_kl            | 0.008836998 |
|    clip_fraction        | 0.076       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0258     |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0121      |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=1297.83 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.3e+03    |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.01010227 |
|    clip_fraction        | 0.0826     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.771     |
|    explained_variance   | -0.209     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0362    |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0187    |
|    value_loss           | 0.015      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1423   |
|    iterations      | 220    |
|    time_elapsed    | 316    |
|    total_timesteps | 450560 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1425        |
|    iterations           | 221         |
|    time_elapsed         | 317         |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.007355948 |
|    clip_fraction        | 0.0732      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.0141      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1426         |
|    iterations           | 222          |
|    time_elapsed         | 318          |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 0.0057864264 |
|    clip_fraction        | 0.0601       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0294      |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.0121       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 223         |
|    time_elapsed         | 319         |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.005428068 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.416      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0189     |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.0203      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1429         |
|    iterations           | 224          |
|    time_elapsed         | 320          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0068090395 |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.7         |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0179      |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.0164      |
|    value_loss           | 0.029        |
------------------------------------------
Eval num_timesteps=460000, episode_reward=1315.10 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.32e+03   |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.00705475 |
|    clip_fraction        | 0.0638     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.184      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0335    |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 0.0479     |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1420   |
|    iterations      | 225    |
|    time_elapsed    | 324    |
|    total_timesteps | 460800 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1422        |
|    iterations           | 226         |
|    time_elapsed         | 325         |
|    total_timesteps      | 462848      |
| train/                  |             |
|    approx_kl            | 0.008077603 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0316     |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0242      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1424        |
|    iterations           | 227         |
|    time_elapsed         | 326         |
|    total_timesteps      | 464896      |
| train/                  |             |
|    approx_kl            | 0.008980427 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0283     |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.028       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1426        |
|    iterations           | 228         |
|    time_elapsed         | 327         |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.010731449 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.0116      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1427         |
|    iterations           | 229          |
|    time_elapsed         | 328          |
|    total_timesteps      | 468992       |
| train/                  |              |
|    approx_kl            | 0.0074411165 |
|    clip_fraction        | 0.0749       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.759       |
|    explained_variance   | -0.0814      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0205      |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.0153      |
|    value_loss           | 0.013        |
------------------------------------------
Eval num_timesteps=470000, episode_reward=1301.44 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.3e+03    |
| time/                   |            |
|    total_timesteps      | 470000     |
| train/                  |            |
|    approx_kl            | 0.00610971 |
|    clip_fraction        | 0.0604     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.585     |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0296    |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0139    |
|    value_loss           | 0.0164     |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1420   |
|    iterations      | 230    |
|    time_elapsed    | 331    |
|    total_timesteps | 471040 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1421        |
|    iterations           | 231         |
|    time_elapsed         | 332         |
|    total_timesteps      | 473088      |
| train/                  |             |
|    approx_kl            | 0.008503137 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.911      |
|    explained_variance   | 0.0571      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.0179      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1423        |
|    iterations           | 232         |
|    time_elapsed         | 333         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.009797072 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0253     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0161      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1424         |
|    iterations           | 233          |
|    time_elapsed         | 334          |
|    total_timesteps      | 477184       |
| train/                  |              |
|    approx_kl            | 0.0058305943 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.598       |
|    explained_variance   | 0.665        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0271      |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.0135       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1427        |
|    iterations           | 234         |
|    time_elapsed         | 335         |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.008249826 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.55       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0304     |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.0201      |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=1290.55 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.010703734 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0351     |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.0484      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1419   |
|    iterations      | 235    |
|    time_elapsed    | 339    |
|    total_timesteps | 481280 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1421        |
|    iterations           | 236         |
|    time_elapsed         | 339         |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.010049164 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | -0.491      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0358     |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.0215      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1424        |
|    iterations           | 237         |
|    time_elapsed         | 340         |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.008224037 |
|    clip_fraction        | 0.0827      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.0204      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1426       |
|    iterations           | 238        |
|    time_elapsed         | 341        |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.01127506 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.936     |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0332    |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.017      |
----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1427         |
|    iterations           | 239          |
|    time_elapsed         | 342          |
|    total_timesteps      | 489472       |
| train/                  |              |
|    approx_kl            | 0.0066470313 |
|    clip_fraction        | 0.0805       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.777       |
|    explained_variance   | 0.389        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.043       |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.0176      |
|    value_loss           | 0.00957      |
------------------------------------------
Eval num_timesteps=490000, episode_reward=1276.02 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.009494255 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.796      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0366     |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.0142      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1420   |
|    iterations      | 240    |
|    time_elapsed    | 346    |
|    total_timesteps | 491520 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1422        |
|    iterations           | 241         |
|    time_elapsed         | 347         |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.009190432 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.0162      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1423        |
|    iterations           | 242         |
|    time_elapsed         | 348         |
|    total_timesteps      | 495616      |
| train/                  |             |
|    approx_kl            | 0.009038441 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0378     |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.0216      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1424         |
|    iterations           | 243          |
|    time_elapsed         | 349          |
|    total_timesteps      | 497664       |
| train/                  |              |
|    approx_kl            | 0.0075590685 |
|    clip_fraction        | 0.0861       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.74        |
|    explained_variance   | 0.76         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0379      |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.0203      |
|    value_loss           | 0.0161       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1426        |
|    iterations           | 244         |
|    time_elapsed         | 350         |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.006716856 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0305     |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0223      |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=1330.25 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.009259131 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0388     |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.0178      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1419   |
|    iterations      | 245    |
|    time_elapsed    | 353    |
|    total_timesteps | 501760 |
-------------------------------

üíæ Saving model...
  Model saved to: run_002/models/comparison_ppo.zip
  VecNormalize saved to: run_002/models/comparison_ppo_vec_normalize.pkl

============================================================
‚úÖ Training complete!
============================================================

============================================================
  Training Dueling DDQN (Zhang et al. 2023 methodology)
============================================================
============================================================
üöÄ Dueling DDQN Training (Zhang et al. 2023 methodology)
============================================================
  Data dir: training_data
  Episodes: 500
  Device: cpu

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  State dim: 38
  Action dim: 7

üèÉ Starting training...
============================================================
  Episode 10/500 | Avg Reward: 8666.21 | Epsilon: 0.904
  Episode 20/500 | Avg Reward: 9733.50 | Epsilon: 0.818
  Episode 30/500 | Avg Reward: 10652.63 | Epsilon: 0.740
  Episode 40/500 | Avg Reward: 11375.44 | Epsilon: 0.669
  Episode 50/500 | Avg Reward: 12040.46 | Epsilon: 0.605
  [EVAL] Episode 50 | Eval Reward: 1393.12
  [BEST] New best model saved!
  Episode 60/500 | Avg Reward: 12623.29 | Epsilon: 0.547
  Episode 70/500 | Avg Reward: 13232.15 | Epsilon: 0.495
  Episode 80/500 | Avg Reward: 13679.00 | Epsilon: 0.448
  Episode 90/500 | Avg Reward: 14032.11 | Epsilon: 0.405
  Episode 100/500 | Avg Reward: 14319.61 | Epsilon: 0.366
  [EVAL] Episode 100 | Eval Reward: 1381.11
  Episode 110/500 | Avg Reward: 14705.68 | Epsilon: 0.331
  Episode 120/500 | Avg Reward: 14706.72 | Epsilon: 0.299
  Episode 130/500 | Avg Reward: 15040.90 | Epsilon: 0.271
  Episode 140/500 | Avg Reward: 15111.61 | Epsilon: 0.245
  Episode 150/500 | Avg Reward: 15237.99 | Epsilon: 0.221
  [EVAL] Episode 150 | Eval Reward: 1239.14
  Episode 160/500 | Avg Reward: 15334.42 | Epsilon: 0.200
  Episode 170/500 | Avg Reward: 15361.54 | Epsilon: 0.181
  Episode 180/500 | Avg Reward: 15611.95 | Epsilon: 0.164
  Episode 190/500 | Avg Reward: 15490.67 | Epsilon: 0.148
  Episode 200/500 | Avg Reward: 15618.81 | Epsilon: 0.134
  [EVAL] Episode 200 | Eval Reward: 1192.38
  Episode 210/500 | Avg Reward: 15515.66 | Epsilon: 0.121
  Episode 220/500 | Avg Reward: 15448.39 | Epsilon: 0.110
  Episode 230/500 | Avg Reward: 15551.73 | Epsilon: 0.099
  Episode 240/500 | Avg Reward: 15663.34 | Epsilon: 0.090
  Episode 250/500 | Avg Reward: 15623.53 | Epsilon: 0.081
  [EVAL] Episode 250 | Eval Reward: 1169.79
  Episode 260/500 | Avg Reward: 15663.05 | Epsilon: 0.073
  Episode 270/500 | Avg Reward: 15594.94 | Epsilon: 0.066
  Episode 280/500 | Avg Reward: 15707.09 | Epsilon: 0.060
  Episode 290/500 | Avg Reward: 15682.62 | Epsilon: 0.054
  Episode 300/500 | Avg Reward: 15603.53 | Epsilon: 0.050
  [EVAL] Episode 300 | Eval Reward: 1198.78
  Episode 310/500 | Avg Reward: 15688.62 | Epsilon: 0.050
  Episode 320/500 | Avg Reward: 15606.29 | Epsilon: 0.050
  Episode 330/500 | Avg Reward: 15564.57 | Epsilon: 0.050
  Episode 340/500 | Avg Reward: 15547.17 | Epsilon: 0.050
  Episode 350/500 | Avg Reward: 15431.12 | Epsilon: 0.050
  [EVAL] Episode 350 | Eval Reward: 1183.00
  Episode 360/500 | Avg Reward: 15450.26 | Epsilon: 0.050
  Episode 370/500 | Avg Reward: 15352.49 | Epsilon: 0.050
  Episode 380/500 | Avg Reward: 15239.17 | Epsilon: 0.050
  Episode 390/500 | Avg Reward: 15212.99 | Epsilon: 0.050
  Episode 400/500 | Avg Reward: 15235.06 | Epsilon: 0.050
  [EVAL] Episode 400 | Eval Reward: 1213.04
  Episode 410/500 | Avg Reward: 15292.71 | Epsilon: 0.050
  Episode 420/500 | Avg Reward: 15137.84 | Epsilon: 0.050
  Episode 430/500 | Avg Reward: 15315.50 | Epsilon: 0.050
  Episode 440/500 | Avg Reward: 15186.39 | Epsilon: 0.050
  Episode 450/500 | Avg Reward: 15201.72 | Epsilon: 0.050
  [EVAL] Episode 450 | Eval Reward: 1188.89
  Episode 460/500 | Avg Reward: 15097.17 | Epsilon: 0.050
  Episode 470/500 | Avg Reward: 15213.88 | Epsilon: 0.050
  Episode 480/500 | Avg Reward: 15161.93 | Epsilon: 0.050
  Episode 490/500 | Avg Reward: 15125.56 | Epsilon: 0.050
  Episode 500/500 | Avg Reward: 15227.10 | Epsilon: 0.050
  [EVAL] Episode 500 | Eval Reward: 1212.83
  Model saved to: run_002/models/comparison_dqn_final.pth

============================================================
‚úÖ Training complete!
  Best eval reward: 1393.12
============================================================

============================================================
  Training LSTM Dueling DDQN (Sequence-aware)
============================================================
============================================================
üß† LSTM Dueling DDQN Training
============================================================
  Data dir: training_data
  Episodes: 800
  Sequence length: 24 hours
  Device: cpu

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  State dim: 38
  Action dim: 7
  Network input shape: (24, 38)

üèÉ Starting LSTM training...
============================================================
  Episode 10/800 | Avg Reward: 8666.37 | Epsilon: 0.904
  Episode 20/800 | Avg Reward: 9667.31 | Epsilon: 0.818
  Episode 30/800 | Avg Reward: 10525.93 | Epsilon: 0.740
  Episode 40/800 | Avg Reward: 11465.05 | Epsilon: 0.669
  Episode 50/800 | Avg Reward: 12000.26 | Epsilon: 0.605
LSTM DQN training failed: unsupported format string passed to list.__format__

======================================================================
  EVALUATING ALL MODELS
======================================================================

============================================================
  Evaluating PPO on test set
============================================================
============================================================
üìä Evaluation on TEST set (paper methodology)
============================================================
  Data dir: training_data
  Model: run_002/models/comparison_ppo.zip

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
Action distribution:
  HOLD           :    88 (1.3%)
  WIDTH=1        :  3440 (50.6%)
  WIDTH=3        :   271 (4.0%)
  WIDTH=5        :  1746 (25.7%)
  WIDTH=10       :   651 (9.6%)
  WIDTH=20       :   193 (2.8%)
  WIDTH=40       :   411 (6.0%)

Results:
  Mean reward: 2162.86 ¬± 23.83
============================================================

============================================================
  Evaluating Dueling DDQN on test set
============================================================
============================================================
üìä Evaluation on TEST set (Dueling DDQN)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)

Action distribution:
  HOLD      :    20 (0.3%)
  WIDTH=1   :  6350 (93.4%)
  WIDTH=2   :   150 (2.2%)
  WIDTH=3   :   280 (4.1%)
  WIDTH=4   :     0 (0.0%)
  WIDTH=5   :     0 (0.0%)
  WIDTH=6   :     0 (0.0%)

Results:
  Mean reward: 2404.25 ¬± 0.00
============================================================

============================================================
  Evaluating LSTM Dueling DDQN on test set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
LSTM DQN evaluation failed: [Errno 2] No such file or directory: 'run_002/models/comparison_lstm_dqn_final.pth'

======================================================================
  FINAL RESULTS SUMMARY
======================================================================

Baselines:
  hold                :     0.00 +/- 0.00
  fixed_width_1       :  2446.67 +/- 0.00
  fixed_width_5       :  1555.95 +/- 0.00
  fixed_width_10      :   255.02 +/- 0.00

Algorithms:
  ppo                 :  2162.86 +/- 23.83
  dqn                 :  2404.25 +/- 0.00
  lstm_dqn            : ERROR - [Errno 2] No such file or directory: 'run_002/models/comparison_lstm_dqn_final.pth'

  Results saved to run_002/visualizations/comparison_results.json

======================================================================
  GENERATING VISUALIZATIONS
======================================================================

============================================================
üìä Visualizing PPO Decisions on Test Set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
  PPO trajectory: min cumulative = 8, final = 2231

============================================================
üìä Visualizing DQN Decisions on Test Set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)

============================================================
üìà Generating Decision Visualization
============================================================
  üìä Saved to: run_002/visualizations/test_decisions.png

  PPO+DQN saved to: run_002/visualizations/test_decisions.png

============================================================
üìä Visualizing LSTM DQN Decisions on Test Set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  LSTM visualization failed: [Errno 2] No such file or directory: 'run_002/models/comparison_lstm_dqn_final.pth'

============================================================
üìà Generating Learning Curves
============================================================
  üìä Saved to: run_002/visualizations/learning_curves.png

  PnL if you invested $1000 at test start:
    (Plotted = single trajectory in viz; Mean = average over 10 eval episodes)
    ppo         plotted: cum $+2230.69  ‚Üí  end $1333.88   |   mean (10ep): cum $+2162.86  ‚Üí  end $1323.73
    dqn         plotted: cum $+2404.25  ‚Üí  end $1359.85   |   mean (10ep): cum $+2404.25  ‚Üí  end $1359.85

  Saved run config to: run_002/RUN_CONFIG.md

======================================================================
  RUN COMPLETE: run_002
======================================================================
