
======================================================================
  RUN: run_001
  Models  -> run_001/models/
  Visuals -> run_001/visualizations/
======================================================================


======================================================================
  RUNNING BASELINES
======================================================================

============================================================
üìä Baseline: HOLD (no LP)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Mean reward: 0.00 ¬± 0.00

============================================================
üìä Baseline: Fixed Width=1 (centered, always reallocate)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Action mapping: width=1 ‚Üí action_id=1
  Mean reward: 8767.36 ¬± 0.00

============================================================
üìä Baseline: Fixed Width=5 (centered, always reallocate)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Action mapping: width=5 ‚Üí action_id=3
  Mean reward: 3199.78 ¬± 0.00

============================================================
üìä Baseline: Fixed Width=10 (centered, always reallocate)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  Action mapping: width=10 ‚Üí action_id=5
  Mean reward: 149.54 ¬± 0.00

======================================================================
  TRAINING ALL MODELS
======================================================================

============================================================
  Training PPO (Xu & Brini 2025 methodology)
============================================================
============================================================
üöÄ Paper-Based Uniswap v3 PPO Training
   Following Xu & Brini (2025) - arXiv:2501.07508
============================================================
  Data dir: training_data
  Parallel environments: 4
  Total timesteps: 500,000
  Action ticks: [0, 1, 3, 5, 10, 20, 40]

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
  Using extended features (36-dim observation, matching DQN)

üèãÔ∏è Creating training environments...
  ‚úÖ 4 training envs, 1 eval env created

üß† Creating PPO model...
  n_steps per env: 512
  batch_size: 256
Using cpu device

üèÉ Starting training...
============================================================
-----------------------------
| time/              |      |
|    fps             | 1808 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1939        |
|    iterations           | 2           |
|    time_elapsed         | 2           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010097633 |
|    clip_fraction        | 0.0568      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -0.408      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0602      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.542       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 2105        |
|    iterations           | 3           |
|    time_elapsed         | 2           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008175905 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -0.336      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.0731      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 2064        |
|    iterations           | 4           |
|    time_elapsed         | 3           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011845153 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -0.666      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0337     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.0534      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=655.14 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 655          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0091711655 |
|    clip_fraction        | 0.0749       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | -0.435       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0325      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.0413       |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1310  |
|    iterations      | 5     |
|    time_elapsed    | 7     |
|    total_timesteps | 10240 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1368        |
|    iterations           | 6           |
|    time_elapsed         | 8           |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010082912 |
|    clip_fraction        | 0.0739      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -0.222      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0209     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.0494      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1451        |
|    iterations           | 7           |
|    time_elapsed         | 9           |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011839666 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -0.172      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.0687      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1461        |
|    iterations           | 8           |
|    time_elapsed         | 11          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.009270478 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -0.000127   |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0412     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.0346      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1489         |
|    iterations           | 9            |
|    time_elapsed         | 12           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0113078505 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | -0.514       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0231      |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0181      |
|    value_loss           | 0.0628       |
------------------------------------------
Eval num_timesteps=20000, episode_reward=917.30 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 917         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010419128 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0348     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.0545      |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1257  |
|    iterations      | 10    |
|    time_elapsed    | 16    |
|    total_timesteps | 20480 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1300        |
|    iterations           | 11          |
|    time_elapsed         | 17          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.009926104 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0212     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.103       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1338        |
|    iterations           | 12          |
|    time_elapsed         | 18          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009923727 |
|    clip_fraction        | 0.0632      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -0.609      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0148     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.134       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1382        |
|    iterations           | 13          |
|    time_elapsed         | 19          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.010277122 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.11       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0307     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0506      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 14          |
|    time_elapsed         | 20          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009562967 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.185      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0423     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.0551      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=1103.80 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.1e+03      |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0112794405 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.598        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.031       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.016       |
|    value_loss           | 0.031        |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1301  |
|    iterations      | 15    |
|    time_elapsed    | 23    |
|    total_timesteps | 30720 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1320        |
|    iterations           | 16          |
|    time_elapsed         | 24          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.010831447 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0418     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0247      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1352        |
|    iterations           | 17          |
|    time_elapsed         | 25          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011966138 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.0989     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0492     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.0426      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1380        |
|    iterations           | 18          |
|    time_elapsed         | 26          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.010096177 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.0794      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0326     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.0256      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1397       |
|    iterations           | 19         |
|    time_elapsed         | 27         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.00838417 |
|    clip_fraction        | 0.0714     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | 0.277      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0256    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 0.0434     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=1100.57 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.1e+03     |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.011948597 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.0897      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.0603      |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 1300  |
|    iterations      | 20    |
|    time_elapsed    | 31    |
|    total_timesteps | 40960 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1324        |
|    iterations           | 21          |
|    time_elapsed         | 32          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.010507988 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0287     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0781      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1344        |
|    iterations           | 22          |
|    time_elapsed         | 33          |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.010343537 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.0585      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1371        |
|    iterations           | 23          |
|    time_elapsed         | 34          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.012188778 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | -0.0629     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0463     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.106       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1395        |
|    iterations           | 24          |
|    time_elapsed         | 35          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.010007858 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | -0.0455     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.0672      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=1165.06 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.17e+03    |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008867178 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.0259      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.016      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.0646      |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1326  |
|    iterations      | 25    |
|    time_elapsed    | 38    |
|    total_timesteps | 51200 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1344        |
|    iterations           | 26          |
|    time_elapsed         | 39          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.012232613 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0512     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.0283      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1355         |
|    iterations           | 27           |
|    time_elapsed         | 40           |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0109382495 |
|    clip_fraction        | 0.0983       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.47        |
|    explained_variance   | -0.752       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0143      |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.0176      |
|    value_loss           | 0.0258       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1376        |
|    iterations           | 28          |
|    time_elapsed         | 41          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.010384717 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.125      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.0357      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1389        |
|    iterations           | 29          |
|    time_elapsed         | 42          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.008897211 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | -0.262      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0221      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=1201.31 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.2e+03      |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0090090055 |
|    clip_fraction        | 0.0922       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.02        |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.0186      |
|    value_loss           | 0.0646       |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1333  |
|    iterations      | 30    |
|    time_elapsed    | 46    |
|    total_timesteps | 61440 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1346         |
|    iterations           | 31           |
|    time_elapsed         | 47           |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0069866474 |
|    clip_fraction        | 0.0668       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.537        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.021       |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.0341       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1365         |
|    iterations           | 32           |
|    time_elapsed         | 47           |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0067305397 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.315        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00971     |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0132      |
|    value_loss           | 0.0549       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1377        |
|    iterations           | 33          |
|    time_elapsed         | 49          |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.008576989 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.0328      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0133     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0977      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1396        |
|    iterations           | 34          |
|    time_elapsed         | 49          |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.010555029 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.194       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0198     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.0365      |
-----------------------------------------
Eval num_timesteps=70000, episode_reward=1232.49 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.23e+03    |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.010260489 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0261     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.0337      |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1353  |
|    iterations      | 35    |
|    time_elapsed    | 52    |
|    total_timesteps | 71680 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1367        |
|    iterations           | 36          |
|    time_elapsed         | 53          |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.008649131 |
|    clip_fraction        | 0.0758      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | -0.269      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0347     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0308      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1377        |
|    iterations           | 37          |
|    time_elapsed         | 54          |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.010796674 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.0167      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1390        |
|    iterations           | 38          |
|    time_elapsed         | 55          |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.011782327 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.119       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.0263      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1404        |
|    iterations           | 39          |
|    time_elapsed         | 56          |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.010799777 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.111       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0329     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.0324      |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=1277.55 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.28e+03   |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.00844975 |
|    clip_fraction        | 0.0971     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0227    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.0353     |
----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1359  |
|    iterations      | 40    |
|    time_elapsed    | 60    |
|    total_timesteps | 81920 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1371        |
|    iterations           | 41          |
|    time_elapsed         | 61          |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.010199311 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.0863      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.0323      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1383         |
|    iterations           | 42           |
|    time_elapsed         | 62           |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0062682573 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.423        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0302      |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 0.0412       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1393        |
|    iterations           | 43          |
|    time_elapsed         | 63          |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.009609346 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.0386      |
-----------------------------------------
Eval num_timesteps=90000, episode_reward=1311.30 +/- 0.00
Episode length: 679.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 679        |
|    mean_reward          | 1.31e+03   |
| time/                   |            |
|    total_timesteps      | 90000      |
| train/                  |            |
|    approx_kl            | 0.00881738 |
|    clip_fraction        | 0.0942     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.237      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0196    |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.0891     |
----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 1358  |
|    iterations      | 44    |
|    time_elapsed    | 66    |
|    total_timesteps | 90112 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1371         |
|    iterations           | 45           |
|    time_elapsed         | 67           |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0111949155 |
|    clip_fraction        | 0.0963       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | -1.49        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0175      |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.0165      |
|    value_loss           | 0.0399       |
------------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1386       |
|    iterations           | 46         |
|    time_elapsed         | 67         |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.00835252 |
|    clip_fraction        | 0.0791     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0273    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.0387     |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1396        |
|    iterations           | 47          |
|    time_elapsed         | 68          |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.010908518 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.0272      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1401        |
|    iterations           | 48          |
|    time_elapsed         | 70          |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.012202229 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.217       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0304     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.0177      |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=1303.61 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.3e+03     |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.010244384 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.0716      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.0234      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1371   |
|    iterations      | 49     |
|    time_elapsed    | 73     |
|    total_timesteps | 100352 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1382       |
|    iterations           | 50         |
|    time_elapsed         | 74         |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.00951806 |
|    clip_fraction        | 0.0904     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | -0.595     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0387    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.016      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1389        |
|    iterations           | 51          |
|    time_elapsed         | 75          |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.007728015 |
|    clip_fraction        | 0.0739      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0219     |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.0356      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1396        |
|    iterations           | 52          |
|    time_elapsed         | 76          |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.009226181 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -0.253      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0379     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.026       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1407        |
|    iterations           | 53          |
|    time_elapsed         | 77          |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.005688018 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0196     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.0326      |
-----------------------------------------
Eval num_timesteps=110000, episode_reward=1285.84 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.007448514 |
|    clip_fraction        | 0.0685      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.0851      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0226     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0423      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1374   |
|    iterations      | 54     |
|    time_elapsed    | 80     |
|    total_timesteps | 110592 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1386        |
|    iterations           | 55          |
|    time_elapsed         | 81          |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.008847814 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.084       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1398        |
|    iterations           | 56          |
|    time_elapsed         | 81          |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.007835534 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0352      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1407         |
|    iterations           | 57           |
|    time_elapsed         | 82           |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.0084764855 |
|    clip_fraction        | 0.0825       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | -0.0785      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0221      |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.044        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1414        |
|    iterations           | 58          |
|    time_elapsed         | 83          |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.010961972 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.178       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0255     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.02        |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=1290.69 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.009124966 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0315     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0148      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1384   |
|    iterations      | 59     |
|    time_elapsed    | 87     |
|    total_timesteps | 120832 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1395         |
|    iterations           | 60           |
|    time_elapsed         | 88           |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0074516856 |
|    clip_fraction        | 0.0709       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.995       |
|    explained_variance   | 0.37         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0248      |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.0226       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1401         |
|    iterations           | 61           |
|    time_elapsed         | 89           |
|    total_timesteps      | 124928       |
| train/                  |              |
|    approx_kl            | 0.0083474675 |
|    clip_fraction        | 0.0788       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.256        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0323      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.022        |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1409         |
|    iterations           | 62           |
|    time_elapsed         | 90           |
|    total_timesteps      | 126976       |
| train/                  |              |
|    approx_kl            | 0.0089094695 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | -0.0725      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0367      |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 0.0271       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1416        |
|    iterations           | 63          |
|    time_elapsed         | 91          |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.006894445 |
|    clip_fraction        | 0.0537      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.952      |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0238      |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=1286.58 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.29e+03     |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0067271576 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.921       |
|    explained_variance   | 0.585        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.019       |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.0146      |
|    value_loss           | 0.0335       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1391   |
|    iterations      | 64     |
|    time_elapsed    | 94     |
|    total_timesteps | 131072 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1397        |
|    iterations           | 65          |
|    time_elapsed         | 95          |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.008876499 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0124     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.0778      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1407        |
|    iterations           | 66          |
|    time_elapsed         | 96          |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.009363346 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.0425      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.039      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.0245      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1418        |
|    iterations           | 67          |
|    time_elapsed         | 96          |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.009224424 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.941      |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0253     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.0297      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1423        |
|    iterations           | 68          |
|    time_elapsed         | 97          |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.012557551 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.0237      |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=1288.22 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.011347929 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.18       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0344     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.0145      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1397   |
|    iterations      | 69     |
|    time_elapsed    | 101    |
|    total_timesteps | 141312 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1403        |
|    iterations           | 70          |
|    time_elapsed         | 102         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.010787879 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | -0.17       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0353     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.0175      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1412        |
|    iterations           | 71          |
|    time_elapsed         | 102         |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.006933069 |
|    clip_fraction        | 0.0697      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.907      |
|    explained_variance   | 0.144       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0268     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0256      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1417        |
|    iterations           | 72          |
|    time_elapsed         | 104         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.008310097 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.952      |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0219     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.0277      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1422        |
|    iterations           | 73          |
|    time_elapsed         | 105         |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.007613318 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.878      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0275     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.0241      |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=1293.21 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.29e+03     |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0063353465 |
|    clip_fraction        | 0.0717       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.81        |
|    explained_variance   | 0.456        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0169      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.0378       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1401   |
|    iterations      | 74     |
|    time_elapsed    | 108    |
|    total_timesteps | 151552 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1407        |
|    iterations           | 75          |
|    time_elapsed         | 109         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.008481935 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0237      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1415        |
|    iterations           | 76          |
|    time_elapsed         | 109         |
|    total_timesteps      | 155648      |
| train/                  |             |
|    approx_kl            | 0.009820372 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0164     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.0751      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1423        |
|    iterations           | 77          |
|    time_elapsed         | 110         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.006250659 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.845      |
|    explained_variance   | -0.416      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0193     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0433      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1431         |
|    iterations           | 78           |
|    time_elapsed         | 111          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0067178085 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.897       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0298      |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.015       |
|    value_loss           | 0.0298       |
------------------------------------------
Eval num_timesteps=160000, episode_reward=1276.21 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.009919274 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0346     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.0226      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1409   |
|    iterations      | 79     |
|    time_elapsed    | 114    |
|    total_timesteps | 161792 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1413        |
|    iterations           | 80          |
|    time_elapsed         | 115         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.008407932 |
|    clip_fraction        | 0.0843      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.946      |
|    explained_variance   | -0.424      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0157      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1421        |
|    iterations           | 81          |
|    time_elapsed         | 116         |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.008604137 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.944      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.0168      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1426        |
|    iterations           | 82          |
|    time_elapsed         | 117         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.005585376 |
|    clip_fraction        | 0.0556      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.941      |
|    explained_variance   | -0.382      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0289     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.0166      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 83          |
|    time_elapsed         | 118         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.007822133 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.0379      |
-----------------------------------------
Eval num_timesteps=170000, episode_reward=1272.89 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.27e+03     |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0069372216 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.841       |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0131      |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.0166       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1410   |
|    iterations      | 84     |
|    time_elapsed    | 121    |
|    total_timesteps | 172032 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1417         |
|    iterations           | 85           |
|    time_elapsed         | 122          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0068330206 |
|    clip_fraction        | 0.0626       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.787       |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0359      |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.0312       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1421        |
|    iterations           | 86          |
|    time_elapsed         | 123         |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.007939958 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 0.165       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0175     |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0297      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 87          |
|    time_elapsed         | 124         |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.010182387 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.975      |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0389     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.0566      |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=1278.85 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.005717312 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.0313      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1412   |
|    iterations      | 88     |
|    time_elapsed    | 127    |
|    total_timesteps | 180224 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1418         |
|    iterations           | 89           |
|    time_elapsed         | 128          |
|    total_timesteps      | 182272       |
| train/                  |              |
|    approx_kl            | 0.0074185655 |
|    clip_fraction        | 0.0698       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.869       |
|    explained_variance   | 0.00105      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.025       |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.0153      |
|    value_loss           | 0.0415       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1422        |
|    iterations           | 90          |
|    time_elapsed         | 129         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.010442028 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0383     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.0119      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1427        |
|    iterations           | 91          |
|    time_elapsed         | 130         |
|    total_timesteps      | 186368      |
| train/                  |             |
|    approx_kl            | 0.009513012 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0422     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1433        |
|    iterations           | 92          |
|    time_elapsed         | 131         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.005594166 |
|    clip_fraction        | 0.057       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | -0.045      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0207     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0258      |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=1309.50 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.31e+03     |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0065601775 |
|    clip_fraction        | 0.0646       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.908       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0311      |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.0209       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1414   |
|    iterations      | 93     |
|    time_elapsed    | 134    |
|    total_timesteps | 190464 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1419        |
|    iterations           | 94          |
|    time_elapsed         | 135         |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.008200961 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.895      |
|    explained_variance   | -0.025      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0385     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.0215      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1424        |
|    iterations           | 95          |
|    time_elapsed         | 136         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.012352828 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.814      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.0187      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 96          |
|    time_elapsed         | 137         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.008098012 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0308      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1433         |
|    iterations           | 97           |
|    time_elapsed         | 138          |
|    total_timesteps      | 198656       |
| train/                  |              |
|    approx_kl            | 0.0075238487 |
|    clip_fraction        | 0.055        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.96        |
|    explained_variance   | 0.425        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0335      |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.0651       |
------------------------------------------
Eval num_timesteps=200000, episode_reward=1310.24 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.31e+03     |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0081476085 |
|    clip_fraction        | 0.061        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.842       |
|    explained_variance   | -1.12        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0269      |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.0118      |
|    value_loss           | 0.021        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1419   |
|    iterations      | 98     |
|    time_elapsed    | 141    |
|    total_timesteps | 200704 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1426        |
|    iterations           | 99          |
|    time_elapsed         | 142         |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.005850213 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.0314      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 100         |
|    time_elapsed         | 143         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.007494198 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0318     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.0194      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 101         |
|    time_elapsed         | 144         |
|    total_timesteps      | 206848      |
| train/                  |             |
|    approx_kl            | 0.010268997 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.968      |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.0134      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1438        |
|    iterations           | 102         |
|    time_elapsed         | 145         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.011513561 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.0167      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0206     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.021       |
-----------------------------------------
Eval num_timesteps=210000, episode_reward=1317.20 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.32e+03    |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.007866866 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.898      |
|    explained_variance   | -0.427      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0158     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0132      |
-----------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 1422   |
|    iterations      | 103    |
|    time_elapsed    | 148    |
|    total_timesteps | 210944 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1426         |
|    iterations           | 104          |
|    time_elapsed         | 149          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0062872358 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.993       |
|    explained_variance   | 0.32         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0173      |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 0.0221       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 105         |
|    time_elapsed         | 150         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.008848238 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0372     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0197      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1434         |
|    iterations           | 106          |
|    time_elapsed         | 151          |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 0.0068395333 |
|    clip_fraction        | 0.0746       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.751       |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0126      |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.0138      |
|    value_loss           | 0.0348       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1438         |
|    iterations           | 107          |
|    time_elapsed         | 152          |
|    total_timesteps      | 219136       |
| train/                  |              |
|    approx_kl            | 0.0068769315 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.852       |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0351      |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 0.0237       |
------------------------------------------
Eval num_timesteps=220000, episode_reward=1283.01 +/- 0.00
Episode length: 679.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 679       |
|    mean_reward          | 1.28e+03  |
| time/                   |           |
|    total_timesteps      | 220000    |
| train/                  |           |
|    approx_kl            | 0.0108581 |
|    clip_fraction        | 0.0993    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.902    |
|    explained_variance   | 0.148     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0283   |
|    n_updates            | 1070      |
|    policy_gradient_loss | -0.0195   |
|    value_loss           | 0.0668    |
---------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1423   |
|    iterations      | 108    |
|    time_elapsed    | 155    |
|    total_timesteps | 221184 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 109         |
|    time_elapsed         | 156         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.006690934 |
|    clip_fraction        | 0.0667      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.813      |
|    explained_variance   | 0.0593      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00478    |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0408      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1434        |
|    iterations           | 110         |
|    time_elapsed         | 157         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.009799505 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.037       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1438        |
|    iterations           | 111         |
|    time_elapsed         | 158         |
|    total_timesteps      | 227328      |
| train/                  |             |
|    approx_kl            | 0.007083919 |
|    clip_fraction        | 0.0829      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0156      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1440        |
|    iterations           | 112         |
|    time_elapsed         | 159         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.007241232 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.912      |
|    explained_variance   | -0.49       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0329     |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.0144      |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=1301.35 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.3e+03     |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.009134872 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.813      |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0265     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0222      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 113    |
|    time_elapsed    | 162    |
|    total_timesteps | 231424 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 114         |
|    time_elapsed         | 163         |
|    total_timesteps      | 233472      |
| train/                  |             |
|    approx_kl            | 0.008121354 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.915      |
|    explained_variance   | -0.428      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0188     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.0167      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1434        |
|    iterations           | 115         |
|    time_elapsed         | 164         |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.010327809 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.0316      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1437        |
|    iterations           | 116         |
|    time_elapsed         | 165         |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.008437389 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0268     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.0178      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1442         |
|    iterations           | 117          |
|    time_elapsed         | 166          |
|    total_timesteps      | 239616       |
| train/                  |              |
|    approx_kl            | 0.0088472925 |
|    clip_fraction        | 0.0822       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.735       |
|    explained_variance   | 0.497        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0333      |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.017       |
|    value_loss           | 0.0298       |
------------------------------------------
Eval num_timesteps=240000, episode_reward=1294.05 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.009662293 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.893      |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0194     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.0636      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 118    |
|    time_elapsed    | 169    |
|    total_timesteps | 241664 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1432         |
|    iterations           | 119          |
|    time_elapsed         | 170          |
|    total_timesteps      | 243712       |
| train/                  |              |
|    approx_kl            | 0.0076382533 |
|    clip_fraction        | 0.0675       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.886       |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0295      |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.0274       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1438        |
|    iterations           | 120         |
|    time_elapsed         | 170         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.006934543 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.022       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1441        |
|    iterations           | 121         |
|    time_elapsed         | 171         |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.015615173 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.0365      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.0235      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1444        |
|    iterations           | 122         |
|    time_elapsed         | 172         |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.009834765 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.0881      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.0103      |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=1294.58 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.009088546 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.0157      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1429   |
|    iterations      | 123    |
|    time_elapsed    | 176    |
|    total_timesteps | 251904 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1434         |
|    iterations           | 124          |
|    time_elapsed         | 177          |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.0061977976 |
|    clip_fraction        | 0.0687       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.845       |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0223      |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.0204       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1436        |
|    iterations           | 125         |
|    time_elapsed         | 178         |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.009033657 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0321     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0226      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1440        |
|    iterations           | 126         |
|    time_elapsed         | 179         |
|    total_timesteps      | 258048      |
| train/                  |             |
|    approx_kl            | 0.007603498 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0199      |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=1271.76 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.27e+03    |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.008852681 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.421       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.0234      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 127    |
|    time_elapsed    | 182    |
|    total_timesteps | 260096 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 128         |
|    time_elapsed         | 183         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.007982827 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0301     |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.0234      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1433         |
|    iterations           | 129          |
|    time_elapsed         | 184          |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 0.0092762895 |
|    clip_fraction        | 0.0883       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.945       |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0277      |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.0199      |
|    value_loss           | 0.0562       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1437         |
|    iterations           | 130          |
|    time_elapsed         | 185          |
|    total_timesteps      | 266240       |
| train/                  |              |
|    approx_kl            | 0.0070876335 |
|    clip_fraction        | 0.0716       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.801       |
|    explained_variance   | -0.692       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0107      |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.028        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1442        |
|    iterations           | 131         |
|    time_elapsed         | 185         |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.009176062 |
|    clip_fraction        | 0.0971      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0396     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.03        |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=1265.18 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.27e+03    |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.008629193 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.982      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0322     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.0176      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1428   |
|    iterations      | 132    |
|    time_elapsed    | 189    |
|    total_timesteps | 270336 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1430         |
|    iterations           | 133          |
|    time_elapsed         | 190          |
|    total_timesteps      | 272384       |
| train/                  |              |
|    approx_kl            | 0.0085511245 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.835       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0359      |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.0173      |
|    value_loss           | 0.0124       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1434        |
|    iterations           | 134         |
|    time_elapsed         | 191         |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.009762777 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0409     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.0155      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1438         |
|    iterations           | 135          |
|    time_elapsed         | 192          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.0076502888 |
|    clip_fraction        | 0.0607       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.89        |
|    explained_variance   | -0.346       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0141      |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.0118       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1440        |
|    iterations           | 136         |
|    time_elapsed         | 193         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.008268617 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.0201      |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=1285.61 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.29e+03     |
| time/                   |              |
|    total_timesteps      | 280000       |
| train/                  |              |
|    approx_kl            | 0.0074231643 |
|    clip_fraction        | 0.0771       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.793       |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0395      |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.0174       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 137    |
|    time_elapsed    | 196    |
|    total_timesteps | 280576 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1431         |
|    iterations           | 138          |
|    time_elapsed         | 197          |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0073730564 |
|    clip_fraction        | 0.0693       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.684       |
|    explained_variance   | 0.576        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0234      |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.0158      |
|    value_loss           | 0.0236       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1433        |
|    iterations           | 139         |
|    time_elapsed         | 198         |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.009568518 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.872      |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.0306      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1437        |
|    iterations           | 140         |
|    time_elapsed         | 199         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.007910265 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0279     |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.0598      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1442        |
|    iterations           | 141         |
|    time_elapsed         | 200         |
|    total_timesteps      | 288768      |
| train/                  |             |
|    approx_kl            | 0.008396825 |
|    clip_fraction        | 0.079       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.521       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.032       |
-----------------------------------------
Eval num_timesteps=290000, episode_reward=1264.61 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.009579871 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0348     |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0384      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1429   |
|    iterations      | 142    |
|    time_elapsed    | 203    |
|    total_timesteps | 290816 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 143         |
|    time_elapsed         | 204         |
|    total_timesteps      | 292864      |
| train/                  |             |
|    approx_kl            | 0.010624094 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.943      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0296     |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1434        |
|    iterations           | 144         |
|    time_elapsed         | 205         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.010576518 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.923      |
|    explained_variance   | 0.0354      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.0111      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1438         |
|    iterations           | 145          |
|    time_elapsed         | 206          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0076598153 |
|    clip_fraction        | 0.0787       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.814       |
|    explained_variance   | 0.379        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0206      |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.016       |
|    value_loss           | 0.0184       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1440         |
|    iterations           | 146          |
|    time_elapsed         | 207          |
|    total_timesteps      | 299008       |
| train/                  |              |
|    approx_kl            | 0.0076250657 |
|    clip_fraction        | 0.0731       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.977       |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0337      |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 0.0184       |
------------------------------------------
Eval num_timesteps=300000, episode_reward=1264.62 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.006782769 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.92       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0257     |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0196      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1428   |
|    iterations      | 147    |
|    time_elapsed    | 210    |
|    total_timesteps | 301056 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 148         |
|    time_elapsed         | 211         |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.009381941 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0335     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.0182      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1434         |
|    iterations           | 149          |
|    time_elapsed         | 212          |
|    total_timesteps      | 305152       |
| train/                  |              |
|    approx_kl            | 0.0073204585 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.756       |
|    explained_variance   | 0.545        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0221      |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.0189      |
|    value_loss           | 0.0272       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1436        |
|    iterations           | 150         |
|    time_elapsed         | 213         |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.010934813 |
|    clip_fraction        | 0.096       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.0554      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1440       |
|    iterations           | 151        |
|    time_elapsed         | 214        |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.00914786 |
|    clip_fraction        | 0.0851     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.828     |
|    explained_variance   | -0.0609    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0366    |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.0219     |
----------------------------------------
Eval num_timesteps=310000, episode_reward=1253.43 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.25e+03     |
| time/                   |              |
|    total_timesteps      | 310000       |
| train/                  |              |
|    approx_kl            | 0.0076421406 |
|    clip_fraction        | 0.0706       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.873       |
|    explained_variance   | 0.597        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0298      |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.0164      |
|    value_loss           | 0.0245       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1429   |
|    iterations      | 152    |
|    time_elapsed    | 217    |
|    total_timesteps | 311296 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 153         |
|    time_elapsed         | 218         |
|    total_timesteps      | 313344      |
| train/                  |             |
|    approx_kl            | 0.009188832 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.211       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.0195      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1434         |
|    iterations           | 154          |
|    time_elapsed         | 219          |
|    total_timesteps      | 315392       |
| train/                  |              |
|    approx_kl            | 0.0072062407 |
|    clip_fraction        | 0.0588       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.845       |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00766     |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.0115       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1437        |
|    iterations           | 155         |
|    time_elapsed         | 220         |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.008988232 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.0156      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1441        |
|    iterations           | 156         |
|    time_elapsed         | 221         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.010605538 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0108     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.0181      |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=1259.76 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.009593535 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.397       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0446     |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.0239      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1428   |
|    iterations      | 157    |
|    time_elapsed    | 225    |
|    total_timesteps | 321536 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 158         |
|    time_elapsed         | 226         |
|    total_timesteps      | 323584      |
| train/                  |             |
|    approx_kl            | 0.008109988 |
|    clip_fraction        | 0.0797      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0431     |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.019       |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1434       |
|    iterations           | 159        |
|    time_elapsed         | 227        |
|    total_timesteps      | 325632     |
| train/                  |            |
|    approx_kl            | 0.00709208 |
|    clip_fraction        | 0.0669     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.669     |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0282    |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.0261     |
----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1436       |
|    iterations           | 160        |
|    time_elapsed         | 228        |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.01181782 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0396    |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 0.0214     |
----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1439         |
|    iterations           | 161          |
|    time_elapsed         | 229          |
|    total_timesteps      | 329728       |
| train/                  |              |
|    approx_kl            | 0.0072261463 |
|    clip_fraction        | 0.071        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.865       |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0334      |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.0172      |
|    value_loss           | 0.0525       |
------------------------------------------
Eval num_timesteps=330000, episode_reward=1257.97 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.26e+03     |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0070938426 |
|    clip_fraction        | 0.0786       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.731       |
|    explained_variance   | -0.0953      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0158      |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.0132      |
|    value_loss           | 0.0337       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1429   |
|    iterations      | 162    |
|    time_elapsed    | 232    |
|    total_timesteps | 331776 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 163         |
|    time_elapsed         | 232         |
|    total_timesteps      | 333824      |
| train/                  |             |
|    approx_kl            | 0.007850867 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.035      |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.0238      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 164         |
|    time_elapsed         | 233         |
|    total_timesteps      | 335872      |
| train/                  |             |
|    approx_kl            | 0.008429978 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.925      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0136      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1436         |
|    iterations           | 165          |
|    time_elapsed         | 235          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0078699365 |
|    clip_fraction        | 0.0782       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.788       |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.042       |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.0112       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1440         |
|    iterations           | 166          |
|    time_elapsed         | 236          |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.0080324225 |
|    clip_fraction        | 0.0932       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.752       |
|    explained_variance   | 0.477        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0355      |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.0131       |
------------------------------------------
Eval num_timesteps=340000, episode_reward=1286.65 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.29e+03     |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0069801994 |
|    clip_fraction        | 0.0802       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.845       |
|    explained_variance   | 0.0433       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0365      |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.0152       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1428   |
|    iterations      | 167    |
|    time_elapsed    | 239    |
|    total_timesteps | 342016 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 168         |
|    time_elapsed         | 240         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.007651751 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0255      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1433         |
|    iterations           | 169          |
|    time_elapsed         | 241          |
|    total_timesteps      | 346112       |
| train/                  |              |
|    approx_kl            | 0.0071738875 |
|    clip_fraction        | 0.0763       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.733       |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0292      |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.0166      |
|    value_loss           | 0.0146       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1436        |
|    iterations           | 170         |
|    time_elapsed         | 242         |
|    total_timesteps      | 348160      |
| train/                  |             |
|    approx_kl            | 0.006567074 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.0237      |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=1277.89 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.008702682 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0373     |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0253      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1425   |
|    iterations      | 171    |
|    time_elapsed    | 245    |
|    total_timesteps | 350208 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 172         |
|    time_elapsed         | 246         |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.008296009 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0275     |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0446      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 173         |
|    time_elapsed         | 247         |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.008480385 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.771      |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.024       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 174         |
|    time_elapsed         | 248         |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.008777189 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0355     |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.0302      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1437        |
|    iterations           | 175         |
|    time_elapsed         | 249         |
|    total_timesteps      | 358400      |
| train/                  |             |
|    approx_kl            | 0.009062609 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.875      |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0315     |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.00949     |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=1249.11 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.25e+03    |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.008500066 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.0098      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 176    |
|    time_elapsed    | 252    |
|    total_timesteps | 360448 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1428         |
|    iterations           | 177          |
|    time_elapsed         | 253          |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0072652954 |
|    clip_fraction        | 0.0767       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.769       |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.023       |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.0169      |
|    value_loss           | 0.0181       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1430         |
|    iterations           | 178          |
|    time_elapsed         | 254          |
|    total_timesteps      | 364544       |
| train/                  |              |
|    approx_kl            | 0.0072375145 |
|    clip_fraction        | 0.0722       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.942       |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0342      |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.0169      |
|    value_loss           | 0.0183       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 179         |
|    time_elapsed         | 255         |
|    total_timesteps      | 366592      |
| train/                  |             |
|    approx_kl            | 0.008722415 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0355     |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.0159      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 180         |
|    time_elapsed         | 256         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.009844767 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0428     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.0133      |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=1238.90 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.24e+03     |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 0.0070664436 |
|    clip_fraction        | 0.0737       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.66        |
|    explained_variance   | 0.605        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0195      |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.0152      |
|    value_loss           | 0.0249       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1425   |
|    iterations      | 181    |
|    time_elapsed    | 260    |
|    total_timesteps | 370688 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1427        |
|    iterations           | 182         |
|    time_elapsed         | 261         |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.008413469 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.868      |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0416     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.0412      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 183         |
|    time_elapsed         | 261         |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.008662622 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.763      |
|    explained_variance   | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0258     |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0189      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1435        |
|    iterations           | 184         |
|    time_elapsed         | 262         |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.011693686 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0339     |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.0247      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1436        |
|    iterations           | 185         |
|    time_elapsed         | 263         |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.008650929 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0296     |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.0164      |
-----------------------------------------
Eval num_timesteps=380000, episode_reward=1230.16 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.23e+03    |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.011927527 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0409     |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.00994     |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 186    |
|    time_elapsed    | 267    |
|    total_timesteps | 380928 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 187         |
|    time_elapsed         | 267         |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.008577489 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0251     |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.0148      |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1431       |
|    iterations           | 188        |
|    time_elapsed         | 268        |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.00770919 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.925     |
|    explained_variance   | -0.12      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0434    |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.0119     |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1433        |
|    iterations           | 189         |
|    time_elapsed         | 270         |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.007160807 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.975      |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.022      |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.0199      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1435         |
|    iterations           | 190          |
|    time_elapsed         | 271          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.0069293827 |
|    clip_fraction        | 0.0671       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.775       |
|    explained_variance   | 0.663        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0234      |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.0131      |
|    value_loss           | 0.0186       |
------------------------------------------
Eval num_timesteps=390000, episode_reward=1236.00 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.24e+03    |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.006447168 |
|    clip_fraction        | 0.0674      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0233     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.0294      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1426   |
|    iterations      | 191    |
|    time_elapsed    | 274    |
|    total_timesteps | 391168 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 192         |
|    time_elapsed         | 275         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.008265397 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0216     |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0237      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 193         |
|    time_elapsed         | 276         |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.011177126 |
|    clip_fraction        | 0.0793      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.0283      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0558      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1434         |
|    iterations           | 194          |
|    time_elapsed         | 276          |
|    total_timesteps      | 397312       |
| train/                  |              |
|    approx_kl            | 0.0079847295 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.882       |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0238      |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.0182      |
|    value_loss           | 0.0374       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1437        |
|    iterations           | 195         |
|    time_elapsed         | 277         |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.010248845 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0365     |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0282      |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=1208.52 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.21e+03    |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.007842977 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0451     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.0125      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1427   |
|    iterations      | 196    |
|    time_elapsed    | 281    |
|    total_timesteps | 401408 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 197         |
|    time_elapsed         | 282         |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.010691484 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.866      |
|    explained_variance   | -0.231      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0348     |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.014       |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1431       |
|    iterations           | 198        |
|    time_elapsed         | 283        |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.00792432 |
|    clip_fraction        | 0.0856     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0287    |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.0184     |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1433        |
|    iterations           | 199         |
|    time_elapsed         | 284         |
|    total_timesteps      | 407552      |
| train/                  |             |
|    approx_kl            | 0.006500332 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.0654      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0456     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0165      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1435         |
|    iterations           | 200          |
|    time_elapsed         | 285          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0111799445 |
|    clip_fraction        | 0.0967       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.827       |
|    explained_variance   | 0.643        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0314      |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 0.0266       |
------------------------------------------
Eval num_timesteps=410000, episode_reward=1211.13 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.21e+03    |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.008683564 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.782      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.0166      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1425   |
|    iterations      | 201    |
|    time_elapsed    | 288    |
|    total_timesteps | 411648 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1428         |
|    iterations           | 202          |
|    time_elapsed         | 289          |
|    total_timesteps      | 413696       |
| train/                  |              |
|    approx_kl            | 0.0058240844 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.586       |
|    explained_variance   | 0.519        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0283      |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.0131      |
|    value_loss           | 0.0257       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 203         |
|    time_elapsed         | 290         |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.008721164 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.229       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.0544      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1432        |
|    iterations           | 204         |
|    time_elapsed         | 291         |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.008008547 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.764      |
|    explained_variance   | -0.225      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.03       |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.0253      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1436        |
|    iterations           | 205         |
|    time_elapsed         | 292         |
|    total_timesteps      | 419840      |
| train/                  |             |
|    approx_kl            | 0.011042163 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0302     |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0203      |
-----------------------------------------
Eval num_timesteps=420000, episode_reward=1226.54 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.23e+03    |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.011077538 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.01       |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0211      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1427   |
|    iterations      | 206    |
|    time_elapsed    | 295    |
|    total_timesteps | 421888 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 207         |
|    time_elapsed         | 296         |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.009972282 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0365     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.00937     |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 208         |
|    time_elapsed         | 297         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.006013913 |
|    clip_fraction        | 0.071       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0371     |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0124      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1433         |
|    iterations           | 209          |
|    time_elapsed         | 298          |
|    total_timesteps      | 428032       |
| train/                  |              |
|    approx_kl            | 0.0071288096 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.881       |
|    explained_variance   | 0.451        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0322      |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.0162      |
|    value_loss           | 0.0181       |
------------------------------------------
Eval num_timesteps=430000, episode_reward=1236.63 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.24e+03     |
| time/                   |              |
|    total_timesteps      | 430000       |
| train/                  |              |
|    approx_kl            | 0.0088496795 |
|    clip_fraction        | 0.0968       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.982       |
|    explained_variance   | 0.491        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0422      |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.0197      |
|    value_loss           | 0.0242       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1423   |
|    iterations      | 210    |
|    time_elapsed    | 302    |
|    total_timesteps | 430080 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1425         |
|    iterations           | 211          |
|    time_elapsed         | 303          |
|    total_timesteps      | 432128       |
| train/                  |              |
|    approx_kl            | 0.0065065334 |
|    clip_fraction        | 0.0637       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.763       |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0473      |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 0.0201       |
------------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 1427       |
|    iterations           | 212        |
|    time_elapsed         | 304        |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.00815681 |
|    clip_fraction        | 0.0777     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.729     |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0333    |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.021      |
----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1429         |
|    iterations           | 213          |
|    time_elapsed         | 305          |
|    total_timesteps      | 436224       |
| train/                  |              |
|    approx_kl            | 0.0079897605 |
|    clip_fraction        | 0.0755       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.703       |
|    explained_variance   | 0.602        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.025       |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.0188      |
|    value_loss           | 0.0224       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 214         |
|    time_elapsed         | 306         |
|    total_timesteps      | 438272      |
| train/                  |             |
|    approx_kl            | 0.008191444 |
|    clip_fraction        | 0.0757      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.857      |
|    explained_variance   | 0.17        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0451     |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.0433      |
-----------------------------------------
Eval num_timesteps=440000, episode_reward=1242.52 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.24e+03    |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.008013165 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | -0.503      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.026      |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.0203      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1422   |
|    iterations      | 215    |
|    time_elapsed    | 309    |
|    total_timesteps | 440320 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1425        |
|    iterations           | 216         |
|    time_elapsed         | 310         |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.009534406 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.813      |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.03       |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.0227      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1427        |
|    iterations           | 217         |
|    time_elapsed         | 311         |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.007817628 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.966      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0398     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.0147      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1429         |
|    iterations           | 218          |
|    time_elapsed         | 312          |
|    total_timesteps      | 446464       |
| train/                  |              |
|    approx_kl            | 0.0073411036 |
|    clip_fraction        | 0.0669       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.732       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.039       |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.0103       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1431        |
|    iterations           | 219         |
|    time_elapsed         | 313         |
|    total_timesteps      | 448512      |
| train/                  |             |
|    approx_kl            | 0.008816021 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0347     |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.0122      |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=1280.83 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.008202035 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.958      |
|    explained_variance   | -0.0592     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0364     |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.0131      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1423   |
|    iterations      | 220    |
|    time_elapsed    | 316    |
|    total_timesteps | 450560 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1424        |
|    iterations           | 221         |
|    time_elapsed         | 317         |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.008995546 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.903      |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0303     |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.0161      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1426         |
|    iterations           | 222          |
|    time_elapsed         | 318          |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 0.0076636905 |
|    clip_fraction        | 0.0741       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.679       |
|    explained_variance   | 0.585        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0225      |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 0.0146       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1428         |
|    iterations           | 223          |
|    time_elapsed         | 319          |
|    total_timesteps      | 456704       |
| train/                  |              |
|    approx_kl            | 0.0062194373 |
|    clip_fraction        | 0.0583       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.626        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0301      |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.0204       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1430         |
|    iterations           | 224          |
|    time_elapsed         | 320          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0063460288 |
|    clip_fraction        | 0.0681       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.788       |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0295      |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.0163      |
|    value_loss           | 0.0278       |
------------------------------------------
Eval num_timesteps=460000, episode_reward=1260.69 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.008657373 |
|    clip_fraction        | 0.0799      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.129       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0306     |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0522      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1422   |
|    iterations      | 225    |
|    time_elapsed    | 323    |
|    total_timesteps | 460800 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1425         |
|    iterations           | 226          |
|    time_elapsed         | 324          |
|    total_timesteps      | 462848       |
| train/                  |              |
|    approx_kl            | 0.0075052734 |
|    clip_fraction        | 0.0856       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.806       |
|    explained_variance   | 0.542        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0308      |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.0185      |
|    value_loss           | 0.0307       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1428         |
|    iterations           | 227          |
|    time_elapsed         | 325          |
|    total_timesteps      | 464896       |
| train/                  |              |
|    approx_kl            | 0.0074971328 |
|    clip_fraction        | 0.0656       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.824       |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0305      |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.0153      |
|    value_loss           | 0.0293       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1429         |
|    iterations           | 228          |
|    time_elapsed         | 326          |
|    total_timesteps      | 466944       |
| train/                  |              |
|    approx_kl            | 0.0064826095 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.847       |
|    explained_variance   | 0.493        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0297      |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.0167      |
|    value_loss           | 0.0118       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 229         |
|    time_elapsed         | 327         |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.006338417 |
|    clip_fraction        | 0.06        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.193       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.0101      |
-----------------------------------------
Eval num_timesteps=470000, episode_reward=1259.74 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.005964766 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.689      |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0234     |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0155      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1423   |
|    iterations      | 230    |
|    time_elapsed    | 330    |
|    total_timesteps | 471040 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1424         |
|    iterations           | 231          |
|    time_elapsed         | 332          |
|    total_timesteps      | 473088       |
| train/                  |              |
|    approx_kl            | 0.0097548645 |
|    clip_fraction        | 0.0839       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.988       |
|    explained_variance   | 0.392        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0376      |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.0185      |
|    value_loss           | 0.0164       |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1427        |
|    iterations           | 232         |
|    time_elapsed         | 332         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.008694446 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0459     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.0181      |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1428         |
|    iterations           | 233          |
|    time_elapsed         | 334          |
|    total_timesteps      | 477184       |
| train/                  |              |
|    approx_kl            | 0.0064851176 |
|    clip_fraction        | 0.0788       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.725       |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0316      |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.0176      |
|    value_loss           | 0.0138       |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 1430         |
|    iterations           | 234          |
|    time_elapsed         | 334          |
|    total_timesteps      | 479232       |
| train/                  |              |
|    approx_kl            | 0.0065323664 |
|    clip_fraction        | 0.068        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.621       |
|    explained_variance   | 0.605        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0167      |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.0166      |
|    value_loss           | 0.0215       |
------------------------------------------
Eval num_timesteps=480000, episode_reward=1276.30 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.28e+03     |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0073343604 |
|    clip_fraction        | 0.0558       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.79        |
|    explained_variance   | 0.339        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.032       |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.0164      |
|    value_loss           | 0.0435       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1422   |
|    iterations      | 235    |
|    time_elapsed    | 338    |
|    total_timesteps | 481280 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1424        |
|    iterations           | 236         |
|    time_elapsed         | 339         |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.005907367 |
|    clip_fraction        | 0.0497      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.674      |
|    explained_variance   | -0.472      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0326     |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0225      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1427        |
|    iterations           | 237         |
|    time_elapsed         | 339         |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.007978438 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0322     |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.0245      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1429        |
|    iterations           | 238         |
|    time_elapsed         | 340         |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.009080581 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0369     |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.0177      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 239         |
|    time_elapsed         | 342         |
|    total_timesteps      | 489472      |
| train/                  |             |
|    approx_kl            | 0.008310704 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.00967     |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=1275.09 +/- 0.00
Episode length: 679.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 679          |
|    mean_reward          | 1.28e+03     |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0072252518 |
|    clip_fraction        | 0.0779       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.737       |
|    explained_variance   | 0.455        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0322      |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.0185      |
|    value_loss           | 0.013        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1423   |
|    iterations      | 240    |
|    time_elapsed    | 345    |
|    total_timesteps | 491520 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1425        |
|    iterations           | 241         |
|    time_elapsed         | 346         |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.008041931 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.891      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0405     |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.0164      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1426        |
|    iterations           | 242         |
|    time_elapsed         | 347         |
|    total_timesteps      | 495616      |
| train/                  |             |
|    approx_kl            | 0.007869644 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0214      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1428        |
|    iterations           | 243         |
|    time_elapsed         | 348         |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.008732286 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0426     |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.0193      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1430        |
|    iterations           | 244         |
|    time_elapsed         | 349         |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.008905042 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.032      |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.0238      |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=1280.26 +/- 0.00
Episode length: 679.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 679         |
|    mean_reward          | 1.28e+03    |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.009247138 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.741      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0484     |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.0167      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 1422   |
|    iterations      | 245    |
|    time_elapsed    | 352    |
|    total_timesteps | 501760 |
-------------------------------

üíæ Saving model...
  Model saved to: run_001/models/comparison_ppo.zip
  VecNormalize saved to: run_001/models/comparison_ppo_vec_normalize.pkl

============================================================
‚úÖ Training complete!
============================================================

============================================================
  Training Dueling DDQN (Zhang et al. 2023 methodology)
============================================================
============================================================
üöÄ Dueling DDQN Training (Zhang et al. 2023 methodology)
============================================================
  Data dir: training_data
  Episodes: 500
  Device: cpu

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  State dim: 38
  Action dim: 7

üèÉ Starting training...
============================================================
  Episode 10/500 | Avg Reward: 2555611.00 | Epsilon: 0.904
  Episode 20/500 | Avg Reward: 2673978.80 | Epsilon: 0.818
  Episode 30/500 | Avg Reward: 3084456.79 | Epsilon: 0.740
  Episode 40/500 | Avg Reward: 3688578.20 | Epsilon: 0.669
  Episode 50/500 | Avg Reward: 4281275.10 | Epsilon: 0.605
  [EVAL] Episode 50 | Eval Reward: 604.66
  [BEST] New best model saved!
  Episode 60/500 | Avg Reward: 4337517.17 | Epsilon: 0.547
  Episode 70/500 | Avg Reward: 4493277.15 | Epsilon: 0.495
  Episode 80/500 | Avg Reward: 3707196.59 | Epsilon: 0.448
  Episode 90/500 | Avg Reward: 4359462.30 | Epsilon: 0.405
  Episode 100/500 | Avg Reward: 4789432.45 | Epsilon: 0.366
  [EVAL] Episode 100 | Eval Reward: 330.95
  Episode 110/500 | Avg Reward: 4921809.00 | Epsilon: 0.331
  Episode 120/500 | Avg Reward: 5416266.68 | Epsilon: 0.299
  Episode 130/500 | Avg Reward: 5748469.69 | Epsilon: 0.271
  Episode 140/500 | Avg Reward: 6445034.28 | Epsilon: 0.245
  Episode 150/500 | Avg Reward: 7374716.97 | Epsilon: 0.221
  [EVAL] Episode 150 | Eval Reward: -35.13
  Episode 160/500 | Avg Reward: 3584035.20 | Epsilon: 0.200
  Episode 170/500 | Avg Reward: 6848496.94 | Epsilon: 0.181
  Episode 180/500 | Avg Reward: 7013641.47 | Epsilon: 0.164
  Episode 190/500 | Avg Reward: 6839508.80 | Epsilon: 0.148
  Episode 200/500 | Avg Reward: 5339327.26 | Epsilon: 0.134
  [EVAL] Episode 200 | Eval Reward: -88.61
  Episode 210/500 | Avg Reward: 1097450.35 | Epsilon: 0.121
  Episode 220/500 | Avg Reward: 1882240.26 | Epsilon: 0.110
  Episode 230/500 | Avg Reward: 7035878.01 | Epsilon: 0.099
  Episode 240/500 | Avg Reward: 118167.98 | Epsilon: 0.090
  Episode 250/500 | Avg Reward: 1761389.68 | Epsilon: 0.081
  [EVAL] Episode 250 | Eval Reward: 3223.40
  [BEST] New best model saved!
  Episode 260/500 | Avg Reward: 8293771.44 | Epsilon: 0.073
  Episode 270/500 | Avg Reward: 8331405.89 | Epsilon: 0.066
  Episode 280/500 | Avg Reward: 8514661.02 | Epsilon: 0.060
  Episode 290/500 | Avg Reward: 7854668.86 | Epsilon: 0.054
  Episode 300/500 | Avg Reward: 6566109.55 | Epsilon: 0.050
  [EVAL] Episode 300 | Eval Reward: 1866.50
  Episode 310/500 | Avg Reward: 7434404.39 | Epsilon: 0.050
  Episode 320/500 | Avg Reward: 7075478.43 | Epsilon: 0.050
  Episode 330/500 | Avg Reward: 6619520.50 | Epsilon: 0.050
  Episode 340/500 | Avg Reward: 6180635.97 | Epsilon: 0.050
  Episode 350/500 | Avg Reward: 7205605.80 | Epsilon: 0.050
  [EVAL] Episode 350 | Eval Reward: 955.62
  Episode 360/500 | Avg Reward: 7194612.96 | Epsilon: 0.050
  Episode 370/500 | Avg Reward: 7838937.98 | Epsilon: 0.050
  Episode 380/500 | Avg Reward: 7518268.82 | Epsilon: 0.050
  Episode 390/500 | Avg Reward: 7407788.74 | Epsilon: 0.050
  Episode 400/500 | Avg Reward: 7612886.71 | Epsilon: 0.050
  [EVAL] Episode 400 | Eval Reward: 955.62
  Episode 410/500 | Avg Reward: 7484506.07 | Epsilon: 0.050
  Episode 420/500 | Avg Reward: 7367693.59 | Epsilon: 0.050
  Episode 430/500 | Avg Reward: 6438759.74 | Epsilon: 0.050
  Episode 440/500 | Avg Reward: 7592314.25 | Epsilon: 0.050
  Episode 450/500 | Avg Reward: 7692182.69 | Epsilon: 0.050
  [EVAL] Episode 450 | Eval Reward: 955.62
  Episode 460/500 | Avg Reward: 7414481.54 | Epsilon: 0.050
  Episode 470/500 | Avg Reward: 7549912.12 | Epsilon: 0.050
  Episode 480/500 | Avg Reward: 7324490.63 | Epsilon: 0.050
  Episode 490/500 | Avg Reward: 7141626.52 | Epsilon: 0.050
  Episode 500/500 | Avg Reward: 7406942.88 | Epsilon: 0.050
  [EVAL] Episode 500 | Eval Reward: 955.62
  Model saved to: run_001/models/comparison_dqn_final.pth

============================================================
‚úÖ Training complete!
  Best eval reward: 3223.40
============================================================

============================================================
  Training LSTM Dueling DDQN (Sequence-aware)
============================================================
============================================================
üß† LSTM Dueling DDQN Training
============================================================
  Data dir: training_data
  Episodes: 800
  Sequence length: 24 hours
  Device: cpu

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  State dim: 38
  Action dim: 7
  Network input shape: (24, 38)

üèÉ Starting LSTM training...
============================================================
  Episode 10/800 | Avg Reward: 2680118.91 | Epsilon: 0.904
  Episode 20/800 | Avg Reward: 3332938.30 | Epsilon: 0.818
  Episode 30/800 | Avg Reward: 4022513.19 | Epsilon: 0.740
  Episode 40/800 | Avg Reward: 4576446.62 | Epsilon: 0.669
  Episode 50/800 | Avg Reward: 5169878.48 | Epsilon: 0.605
LSTM DQN training failed: unsupported format string passed to list.__format__

======================================================================
  EVALUATING ALL MODELS
======================================================================

============================================================
  Evaluating PPO on test set
============================================================
============================================================
üìä Evaluation on TEST set (paper methodology)
============================================================
  Data dir: training_data
  Model: run_001/models/comparison_ppo.zip

üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
Action distribution:
  HOLD           :  1533 (22.5%)
  WIDTH=1        :  4021 (59.1%)
  WIDTH=3        :   292 (4.3%)
  WIDTH=5        :   234 (3.4%)
  WIDTH=10       :    23 (0.3%)
  WIDTH=20       :   593 (8.7%)
  WIDTH=40       :   104 (1.5%)

Results:
  Mean reward: 2010.14 ¬± 66.90
============================================================

============================================================
  Evaluating Dueling DDQN on test set
============================================================
============================================================
üìä Evaluation on TEST set (Dueling DDQN)
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)

Action distribution:
  HOLD      :     0 (0.0%)
  WIDTH=1   :  5810 (85.4%)
  WIDTH=2   :     0 (0.0%)
  WIDTH=3   :   990 (14.6%)
  WIDTH=4   :     0 (0.0%)
  WIDTH=5   :     0 (0.0%)
  WIDTH=6   :     0 (0.0%)

Results:
  Mean reward: 7351.68 ¬± 0.00
============================================================

============================================================
  Evaluating LSTM Dueling DDQN on test set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
LSTM DQN evaluation failed: [Errno 2] No such file or directory: 'run_001/models/comparison_lstm_dqn_final.pth'

======================================================================
  FINAL RESULTS SUMMARY
======================================================================

Baselines:
  hold                :     0.00 +/- 0.00
  fixed_width_1       :  8767.36 +/- 0.00
  fixed_width_5       :  3199.78 +/- 0.00
  fixed_width_10      :   149.54 +/- 0.00

Algorithms:
  ppo                 :  2010.14 +/- 66.90
  dqn                 :  7351.68 +/- 0.00
  lstm_dqn            : ERROR - [Errno 2] No such file or directory: 'run_001/models/comparison_lstm_dqn_final.pth'

  Results saved to run_001/visualizations/comparison_results.json

======================================================================
  GENERATING VISUALIZATIONS
======================================================================

============================================================
üìä Visualizing PPO Decisions on Test Set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
  PPO trajectory: min cumulative = 8, final = 2210

============================================================
üìä Visualizing DQN Decisions on Test Set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)

============================================================
üìà Generating Decision Visualization
============================================================
  üìä Saved to: run_001/visualizations/test_decisions.png

  PPO+DQN saved to: run_001/visualizations/test_decisions.png

============================================================
üìä Visualizing LSTM DQN Decisions on Test Set
============================================================
üîÑ Preparing hourly data with technical indicators...
  Pool fee: 0.05%
  Tick spacing: 10
  üìä 6803 hourly candles
  üìà Computed 31 features (22 technical + 9 trend)
‚úÖ Data preparation complete!
üéÆ Action Space: 7 actions
   - HOLD (action=0)
   - Widths: [1, 3, 5, 10, 20, 40] ticks (always centered)
  LSTM visualization failed: [Errno 2] No such file or directory: 'run_001/models/comparison_lstm_dqn_final.pth'

============================================================
üìà Generating Learning Curves
============================================================
  üìä Saved to: run_001/visualizations/learning_curves.png

  PnL if you invested $1000 at test start:
    (Plotted = single trajectory in viz; Mean = average over 10 eval episodes)
    ppo         plotted: cum $+2209.62  ‚Üí  end $1330.72   |   mean (10ep): cum $+2010.14  ‚Üí  end $1300.87
    dqn         plotted: cum $+7351.68  ‚Üí  end $2100.36   |   mean (10ep): cum $+7351.68  ‚Üí  end $2100.36

  Saved run config to: run_001/RUN_CONFIG.md

======================================================================
  RUN COMPLETE: run_001
======================================================================
